[{"path":"https://seraphinem.github.io/quallmer/articles/getting-started.html","id":"basic-usage","dir":"Articles","previous_headings":"","what":"Basic usage","title":"Getting started with quallmer","text":"quallmer package developed using R. Please make sure recent version R RStudio installed computer. new R RStudio, can find great free--charge 1.5h introduction R RStudio instats. get started quallmer, first need install package GitHub. , can load package begin using functions.","code":"# If you don't have pak installed yet, uncomment and run the following line: # install.packages(\"pak\") # Then, install quallmer using pak: pak::pak(\"SeraphineM/quallmer\") library(quallmer) #> Loading required package: ellmer"},{"path":"https://seraphinem.github.io/quallmer/articles/getting-started.html","id":"overview-of-tutorials","dir":"Articles","previous_headings":"","what":"Overview of tutorials","title":"Getting started with quallmer","text":"using large language models, users need sign API key LLM provider, openai, download open-source model like Ollama. quallmer package supports multiple LLM providers ellmer package, allowing users choose one best fits needs. Using annotate(), users can generate structured, interpretable outputs powered large language models (LLMs). package includes library predefined tasks common qualitative coding needs, sentiment analysis, thematic coding, stance detection. also allows users create custom annotation tasks tailored specific research questions data types using task(). tutorials guide following topics: Signing OpenAI API key: tutorial guide process obtaining API key OpenAI, necessary using OpenAI’s LLMs quallmer package. Working open-source Ollama model: tutorial demonstrate use quallmer package open-source Ollama model qualitative coding tasks. Using predefined tasks: illustrations show utilize library predefined annotation tasks available quallmer package perform common qualitative coding tasks efficiently. Creating custom tasks: tutorial walk process defining custom annotation tasks using task() function, allowing tailor LLM’s output specific research needs. Using Agreement App: tutorial introduce Agreement App manually code data, check LLM annotations, calculate interrater reliability user-friendly intuitive interface. hope tutorials help get started quallmer package empower leverage large language models qualitative research projects!","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/overview.html","id":"predefined-tasks","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Predefined tasks","title":"Overview of predefined tasks","text":"quallmer package currently includes following predefined tasks: wish create custom tasks tailored specific research questions data types, can use task() function. function allows specify system prompt output structure, enabling customize annotation process according needs. tutorial create custom tasks, please refer custom tasks tutorial.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_fact.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Fact checking of claims","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_fact.html","id":"using-annotate-for-fact-checking-of-claims-in-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for fact checking of claims in texts","title":"Example: Fact checking of claims","text":"","code":"# Apply predefined fact checking task with task_fact() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_fact(),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Fact-checking' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 1 -> 3 | ■■■■■■■■■■■■■■■■■■■■■■■           75% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_fact.html","id":"using-annotate-for-fact-checking-with-a-specific-number-of-claims-to-check","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for fact checking with a specific number of claims to check","title":"Example: Fact checking of claims","text":"text political speech, often includes aspirational rhetorical statements rather factual claims. Transfer Power People: claim power transferred Washington people rhetorical factual, systemic changes governance complex solely determined change administration. American Carnage: depiction U.S. suffering widespread ‘carnage’ exaggerated portrayal may accurately reflect overall state nation. America First Policy: ‘America First’ policy stated goal, implications effectiveness subject debate interpretation, making potentially misleading simplification complex international relations economic policies. speech contains several misleading inaccurate claims. Historical Inaccuracies: claim Panama Canal ‘given’ China misleading. Panama Canal transferred Panama, China, Torrijos-Carter Treaties. Policy Claims: Many policy claims, ending Green New Deal revoking electric vehicle mandate, speculative lack context feasibility details. Election Results: assertion winning seven swing states popular vote millions lacks verification context, presented without evidence. example, demonstrated use annotate() function task_fact() fact-check claims corpus innaugural speeches. results include truth score, identified misleading topics, explanations claim evaluated. amount claims check can adjusted using max_topics parameter task_fact() function. Now can apply approach texts fact-checking purposes!","code":"# Apply predefined fact checking task with task_fact() in the annotate() function result_claims <- annotate(data_corpus_inaugural, task = task_fact(max_topics = 3),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Fact-checking' using model: gpt-4o ## [working] (0 + 0) -> 1 -> 3 | ■■■■■■■■■■■■■■■■■■■■■■■           75% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_ideology.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Ideology detection","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_ideology.html","id":"using-annotate-for-ideological-scaling-of-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for ideological scaling of texts","title":"Example: Ideology detection","text":"","code":"# Define ideological dimension dimension <- \"inclusive–exclusive\" # Provide definition for the dimension definition <- \"Inclusive language emphasizes equal rights, diversity, pluralism,  and protection of minorities, whereas exclusive language emphasizes exclusion  of groups, national homogeneity, and restricting rights.\" # Apply predefined ideology task with task_ideology() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_ideology(dimension, definition),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Ideological scaling' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_ideology.html","id":"adjusting-the-ideology-scaling-task","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Adjusting the ideology scaling task","title":"Example: Ideology detection","text":"can customize ideological scaling task defining task task() (detailed explanation, see “Defining custom tasks” tutorial). example, might like change scale 0-10 -5 +5. example, demonstrated use task_ideology() scaling texts regarding ideological position specified dimension. also showed customize task using task() function tailored annotation needs, e.g., changing scale 0-10 -5 +5. Now can apply techniques text data ideological analysis!","code":"custom_ideology <- task(     name = \"Ideological scaling\",     system_prompt = paste0(       \"You are an expert political scientist performing ideological text scaling.\",       \"Task:\",       \"- Read each short text carefully.\",       \"- Place the text on a -5 to +5 scale for the following ideological dimension: \",       dimension,        definition     ),     type_def = ellmer::type_object(       score       = ellmer::type_integer(\"Ideological position on the specified dimension (0–10, where -5 = first pole, +5 = second pole)\"),       explanation = ellmer::type_string(\"Brief justification for the assigned score, referring to specific elements in the text\")     ),     input_type = \"text\"   ) # Apply the custom task custom_result <- annotate(data_corpus_inaugural, task = custom_ideology,                            chat_fn = chat_openai, model = \"gpt-4o\",                           api_args = list(temperature = 0)) ## Running task 'Ideological scaling' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_salience.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Salience of topics","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_salience.html","id":"using-annotate-for-salience-of-any-topics-discussed-in-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for salience of ANY topics discussed in texts","title":"Example: Salience of topics","text":"American Values Ideals: speech frequently references foundational American principles liberty, democracy, Constitution, emphasizing enduring importance. Equality Civil Rights: strong focus equality, references historical movements ongoing journey towards equal rights citizens, including women LGBTQ+ individuals. Collective Action Unity: text emphasizes need unity collective action address modern challenges, highlighting importance working together nation. Economic Social Progress: speech discusses economic recovery, importance strong middle class, need social programs like Medicare Social Security. Role Government: discussion role government ensuring fair play market, protecting citizens, adapting new challenges, balancing individual freedoms collective needs. 2017-Trump Transfer Power People , America First Policy , National Unity Patriotism , Economic Rebuilding Job Creation, Critique Political Establishment Transfer Power People: speech emphasizes returning power Washington citizens, highlighting central theme. America First Policy: repeated focus prioritizing American interests trade, immigration, foreign policy underscores importance. National Unity Patriotism: call unity patriotism recurring theme, references shared values protection. Economic Rebuilding Job Creation: significant emphasis rebuilding infrastructure creating jobs, reflecting salience. Critique Political Establishment: speech critiques political establishment failing people, making notable topic. 2021-Biden Unity , Democracy , Challenges facing America , American values ideals , Historical context legacy Unity: speech emphasizes unity central theme, repeatedly calling Americans come together face challenges heal divisions. Democracy: Democracy highlighted precious fragile cause prevailed, references peaceful transfer power people. Challenges facing America: text discusses various challenges pandemic, racial justice, political extremism, climate change, emphasizing need address issues. American values ideals: speech frequently references values like liberty, dignity, truth, framing core American identity. Historical context legacy: speech draws historical events figures, Civil War past presidents, contextualize current challenges inspire action. 2025-Trump American Sovereignty Nationalism , Government Reform Efficiency , Immigration Border Security , Economic Policies Energy Independence, National Unity Patriotism American Sovereignty Nationalism: speech emphasizes reclaiming sovereignty, restoring national pride, making America respected globally, central themes throughout. Government Reform Efficiency: strong focus reforming government structures, ending corruption, restoring competence, highlighted mentions new departments executive orders. Immigration Border Security: text discusses halting illegal entry, reinstating policies, designating cartels terrorist organizations, indicating significant emphasis border security. Economic Policies Energy Independence: speech outlines plans economic revival, energy independence, manufacturing, specific policies like drilling ending Green New Deal. National Unity Patriotism: speech frequently references national unity, patriotism, historical achievements, aiming inspire unify audience. Economy: text frequently discusses economic themes, economic recovery, prosperity, importance rising middle class. emphasizes need infrastructure, fair markets, economic vitality. Environment: clear emphasis climate change sustainable energy, highlighting need lead transition sustainable energy sources preserve natural resources. Foreign Policy: text addresses America’s role global peace, alliances, supporting democracy worldwide, emphasizing engagement peaceful resolution differences. Education: importance education mentioned context training workers equipping children future, indicating role economic social progress. Health: Health care mentioned context reducing costs ensuring security dignity citizens, references Medicare Medicaid. 2017-Trump economy , foreign policy, education text primarily focuses economy, discussing jobs, factories, wealth redistribution, mentioned frequently significant emphasis. Foreign policy also key topic, references trade, alliances, military actions. Education mentioned context system failing students, making notable less emphasized topic. 2021-Biden health , economy , environment , foreign policy, education Health: text emphasizes impact pandemic, mentioning virus significant challenge, loss lives, need face pandemic nation. Economy: mention job losses business closures, highlighting economic challenges need rebuild middle class. Environment: text refers ‘cry survival planet,’ indicating environmental concerns. Foreign Policy: text discusses America’s role world, repairing alliances, engaging globally. Education: Briefly mentioned context teaching children safe schools. 2025-Trump foreign policy, economy , health , education , environment Foreign Policy: text frequently discusses international relations, border security, actions foreign entities, emphasizing importance America’s global standing actions like reclaiming Panama Canal. Economy: Economic issues highlighted mentions inflation, energy policies, manufacturing, tariffs, indicating focus economic revitalization. Health: speech references public health system intention end chronic disease epidemic, showing concern health-related issues. Education: education system criticized teaching children negatively, suggesting need reform. Environment: Environmental policies mentioned, particularly context ending Green New Deal energy production, though less prominently topics.","code":"# Apply predefined salience task with task_salience() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_salience(),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Salience (ranked topics)' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% # Define a list of topics to focus on topics <- c(\"economy\", \"health\", \"education\", \"environment\", \"foreign policy\") # Apply predefined salience task with task_salience() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_salience(topics),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Salience (ranked topics)' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 1 -> 3 | ■■■■■■■■■■■■■■■■■■■■■■■           75% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_salience.html","id":"adjusting-the-task_salience-so-it-also-returns-the-stance-for-each-topic","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Adjusting the task_salience() so it also returns the stance for each topic","title":"Example: Salience of topics","text":"example, demonstrated use task_salience() identifying ranking topics discussed texts, without predefined list topics. Additionally, showed customize task include stance classification topic. showcases flexibility annotate() function task framework quallmer various text analysis tasks.","code":"# Customizing the task to include the stance for each topic custom_task <- task(   name = \"Salience and stance of topics\",   system_prompt = paste(     \"You are an expert analysing the content of texts.\",     \"\",     \"Task:\",     \"- Read the text carefully.\",     \"- Identify and rank the salience of the following topics: economy, health, education, environment, foreign policy.\",     \"- For each topic mentioned, assign a stance as one of the following:\",     \"  pro, neutral, or contra.\",     \"- Append the stance directly after each topic name in the form 'topic: stance'.\",     \"- Return all topic:stance entries in descending order of salience.\",     \"- Separate entries with commas when presenting them in a list.\",     \"\",     \"Do not infer information that is not in the text.\",     \"Base all evaluations solely on the language and arguments in the document.\",     \"\",     \"Output:\",     \"- `topic_stance`: a ranked list of topic labels with stance labels appended (e.g., 'economy: pro', 'health: contra').\",     \"- `explanation`: a brief justification explaining why the topics were ordered and how stance was determined.\",     sep = \"\\n\"   ),   type_def = ellmer::type_object(     topic_stance = ellmer::type_array(       ellmer::type_string(\"Topic and stance label combined (e.g., 'economy: pro'), ranked by salience.\")     ),     explanation = ellmer::type_string(       \"Brief justification for the salience ordering and stance classification.\"     )   ),   input_type = \"text\" )  # Apply the customized task in the annotate() function custom_result <- annotate(data_corpus_inaugural, task = custom_task,                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Salience and stance of topics' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_sentiment.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Sentiment analysis","text":"","code":"library(quallmer) ## Loading required package: ellmer #Example texts texts <- c( \"This is wonderful!\", \"I really dislike this approach.\", \"The results are somewhat disappointing.\", \"Absolutely fantastic work!\" )"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_sentiment.html","id":"using-annotate-for-predefined-sentiment-analysis-of-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for predefined sentiment analysis of texts","title":"Example: Sentiment analysis","text":"","code":"# Apply predefined sentiment task with task_sentiment() in the annotate() function result <- annotate(texts, task = task_sentiment(),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Sentiment analysis' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_sentiment.html","id":"adjusting-the-sentiment-task","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Adjusting the sentiment task","title":"Example: Sentiment analysis","text":"can customize sentiment analysis task defining task task() (detailed explanation, see “Defining custom tasks” tutorial). example, might want include additional field confidence level. , might want change scoring scale 5-point Likert scale. way, can easily adapt sentiment analysis task fit specific research needs!","code":"custom_sentiment <- task(   name = \"Custom sentiment analysis\",   system_prompt = \"You are an expert annotator. Rate the sentiment of each text from -1 (very negative) to 1 (very positive), briefly explain why, and provide a confidence level from 0 to 1.\",   type_def = ellmer::type_object(     score = ellmer::type_number(\"Sentiment score between -1 (very negative) and 1 (very positive)\"),     explanation = ellmer::type_string(\"Brief explanation of the rating\"),     confidence = ellmer::type_number(\"Confidence level from 0 to 1\")   ),   input_type = \"text\" ) # Apply the custom sentiment task custom_result <- annotate(texts, task = custom_sentiment,                            chat_fn = chat_openai, model = \"gpt-4o\",                           api_args = list(temperature = 0)) ## Running task 'Custom sentiment analysis' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% likert_sentiment <- task(   name = \"Likert scale sentiment analysis\",   system_prompt = \"You are an expert annotator. Rate the sentiment of each text on a scale from 1 (very negative) to 5 (very positive) and briefly explain why.\",   type_def = ellmer::type_object(     score = ellmer::type_number(\"Sentiment score between 1 (very negative) and 5 (very positive)\"),     explanation = ellmer::type_string(\"Brief explanation of the rating\")   ),   input_type = \"text\" ) # Apply the Likert scale sentiment task likert_result <- annotate(texts, task = likert_sentiment,                            chat_fn = chat_openai, model = \"gpt-4o\",                           api_args = list(temperature = 0)) ## Running task 'Likert scale sentiment analysis' using model: gpt-4o"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_stance.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Stance detection","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_stance.html","id":"using-annotate-for-stance-detection-of-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for stance detection of texts","title":"Example: Stance detection","text":"","code":"# Define topic of interest topic <- \"Climate Change\" # Apply predefined stance task with task_stance() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_stance(topic),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Stance detection' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_stance.html","id":"adjusting-the-stance-detection-task","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Adjusting the stance detection task","title":"Example: Stance detection","text":"can customize stance detection task defining task task() (detailed explanation, see “Defining custom tasks” tutorial). example, might want include additional field confidence level. , might want LLM extract specific arguments supporting stance. Acknowledgment threat: text states failing address climate change betray future generations. Scientific consensus: mentions overwhelming judgment science regarding climate change. Economic environmental leadership: text argues leading transition sustainable energy maintain economic vitality preserve natural resources. Emphasis economic growth job creation without reference environmental impact. Focus national interests infrastructure development without mentioning sustainability. Absence direct reference climate change environmental policies. text describes climate change ‘crisis,’ indicating recognition severity. calls boldness action resolve ‘cascading crises,’ includes climate change. speech emphasizes responsibility pass better world future generations, implying need environmental stewardship. Declares national energy emergency increase drilling fossil fuel use. Ends Green New Deal revokes electric vehicle mandate. Emphasizes using country’s oil gas reserves boost economy. example, demonstrated use stance() task stance detection texts regarding “Climate Change”. also showed customize task include additional fields confidence level key arguments supporting stance. Now turn explore stance detection texts topics interest!","code":"custom_stance <- task(   name = \"Custom stance detection\",   system_prompt = paste0(     \"You are an expert annotator. Read each short text carefully and determine its stance towards \",     topic,     \". Classify the stance as Pro, Neutral, or Contra, provide a brief explanation for your classification, and indicate your confidence level from 0 to 1.\"   ),   type_def = ellmer::type_object(     stance = ellmer::type_string(\"Stance towards the topic: Pro, Neutral, or Contra\"),     explanation = ellmer::type_string(\"Brief explanation of the classification\"),     confidence = ellmer::type_number(\"Confidence level from 0 to 1\")   ),   input_type = \"text\" ) # Apply the custom stance task custom_result <- annotate(data_corpus_inaugural, task = custom_stance,                            chat_fn = chat_openai, model = \"gpt-4o\",                           api_args = list(temperature = 0)) ## Running task 'Custom stance detection' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% argument_stance <- task(   name = \"Argument-based stance detection\",   system_prompt = paste0(     \"You are an expert annotator. Read each short text carefully and determine its stance towards \",     topic,     \". Classify the stance as Pro, Neutral, or Contra, provide a brief explanation for your classification, and list up to three key arguments supporting the stance.\"   ),   type_def = ellmer::type_object(     stance = ellmer::type_string(\"Stance towards the topic: Pro, Neutral, or Contra\"),     explanation = ellmer::type_string(\"Brief explanation of the classification\"),     arguments = ellmer::type_string(\"Key arguments supporting the stance\")   ),   input_type = \"text\" ) # Apply the argument-based stance task argument_result <- annotate(data_corpus_inaugural, task = argument_stance,                              chat_fn = chat_openai, model = \"gpt-4o\",                             api_args = list(temperature = 0)) ## Running task 'Argument-based stance detection' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"launching-the-agreement-app","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Launching the Agreement App","title":"Using the Agreement App","text":"launch Agreement App, can use agreement_app() function quallmer package. Make sure package loaded R environment. , simply call agreement_app() function. open Agreement App new window tab web browser.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"using-the-agreement-app-for-manual-coding","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Using the Agreement App for manual coding","title":"Using the Agreement App","text":"Agreement App launched, can start uploading dataset. app supports .csv .rds file formats. uploading data, can select column containing content (e.g., texts, images, etc.) want manually assess. using app, can manually assign score comments text item based coding scheme. can also save example sentences text item help remember coding decisions later illustrative examples research.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"reviewing-llm-generated-annotations","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Reviewing LLM-generated annotations","title":"Using the Agreement App","text":"previously used quallmer package generate annotations using large language models (LLMs), can upload annotations Agreement App review. app allows check LLM-generated codes alongside justifications provided model. can decide whether accept annotations valid invalid, modify based assessment adding comments example sentences.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"saving-your-coding-decisions","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Saving your coding decisions","title":"Using the Agreement App","text":"app provides intuitive interface navigating data making coding decisions. coding decisions saved automatically, find newly created folder named “agreement” working directory.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"calculating-agreement-scores","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Calculating agreement scores","title":"Using the Agreement App","text":"completing manual coding reviewing LLM-generated annotations, Agreement App provides functionality calculate intercoder reliability scores. can choose various metrics, Krippendorff’s alpha, Cohen’s Fleiss’ kappa, assess agreement different coders manual codes LLM annotations , shown , multiple LLM runs. app also provides interpretation guidelines help understand results.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"calculating-agreement-scores-without-the-app","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Calculating agreement scores without the App","title":"Using the Agreement App","text":"addition using Agreement App, can also calculate agreement scores programmatically using agreement() function quallmer package. function allows specify dataset, column containing unit IDs, columns containing coder annotations. ’s example use agreement() function: return calculated agreement scores based specified coders LLM runs.","code":"results <- agreement(   data        = your_data,   unit_id_col = \"doc_id\",   coder_cols  = c(\"coder1\", \"coder2\", \"llm_run1\", \"llm_run2\") ) results"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/customtask.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Loading packages and data","title":"Defining custom tasks","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/customtask.html","id":"defining-a-custom-prompt","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Defining a custom prompt","title":"Defining custom tasks","text":"Defining prompts crucial step creating custom tasks. prompt guides LLM interpret input data kind output generate. example, create prompt instructs LLM score documents based alignment political left ideologies. Prompts can much longer complex depending task hand. Prompts clear specific ensure LLM understands task requirements.","code":"prompt <- \"Score the following document on a scale of how much it aligns with the political left. The political left is defined as groups which advocate for social equality, government intervention in the economy, and progressive policies. Use the following metrics: SCORING METRIC: 3 : extremely left 2 : very left 1 : slightly left 0 : not at all left\""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/customtask.html","id":"defining-the-structure-of-the-response-with-define_task","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Defining the structure of the response with define_task()","title":"Defining custom tasks","text":"task() function allows us specify expected structure LLM’s response. following important arguments users need specify: name: descriptive name task. system_prompt: prompt guides LLM perform task. type_def: Defines expected structure response using ellmers type specifications type_object(), type_array(), etc. information use ellmer’s type specifications, please refer ellmer documentation type specifications.","code":"# Define the custom task using task() ideology_scores <- task(   name = \"Score Political Left Alignment\",   system_prompt = prompt,   type_def = type_object(     score = type_number(\"Score\"),     explanation = type_string(\"Explanation\")   ),   input_type = \"text\" )"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/customtask.html","id":"applying-the-custom-task-to-the-corpus","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Applying the custom task to the corpus","title":"Defining custom tasks","text":"step similar applying predefined tasks using annotate() function. , use annotate() function apply custom task sample corpus inaugural speeches. specify LLM use (case, openai’s gpt-4o model) additional API arguments needed. example, set temperature 0 deterministic outputs, improving consistency scoring across multiple runs therefore increasing reliability. Now successfully created applied custom annotation task using quallmer package! can modify prompt response structure suit specific research needs.","code":"# Apply the custom task to the inaugural speeches corpus result <- annotate(data_corpus_inaugural, task = ideology_scores,                    chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Score Political Left Alignment' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/openai.html","id":"precautions","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Precautions","title":"Signing up for an openai API key","text":"Closed LLMs OpenAI’s GPT-4o free use require subscription. also come ethical concerns risks, especially comes data privacy security. Therefore, always aware data use potential consequences analysis make sure enable necessary safeguards protect privacy security. addition, aware license use OpenAI models comes along adhering specific regulations avoid misuse. Therefore, always aware data use potential consequences analysis make sure enable necessary safeguards protect privacy security.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/openai.html","id":"setting-up-an-api-key-for-openai-models","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Setting up an API key for openai models","title":"Signing up for an openai API key","text":"openai API provides access various called closed models (fee-based, open-source). services available (example, Claude Google Gemini, etc.) require slightly different set-ups. can find information pricing openai .","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/openai.html","id":"to-use-the-openai-api-please-follow-these-steps","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Setting up an API key for openai models","what":"To use the openai API, please follow these steps:","title":"Signing up for an openai API key","text":"Go openai playground: https://platform.openai.com/playground> Click Sign top right corner Fill details confirm Sign Click now Settings icon top right corner Go Billing provide billing information (otherwise won’t work!) billing information complete, can create new project (top left corner, click Default Project) Click Dashboard top right corner Click API keys left side panel bottom Click Create new secret key Copy key close window copied key, save somewhere safe accessible. security reasons, won’t able view openai account. lose , need regenerate . Keep API key safe share others. suspect key compromised, can regenerate dashboard. aware charged usage API via key.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/openai.html","id":"configuring-your-openai-api-key-in-rstudio","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Setting up an API key for openai models","what":"Configuring your openai API key in RStudio","title":"Signing up for an openai API key","text":"interact openai API, ’s required valid OPENAI_API_KEY environment variable R. can establish environment variable globally including -called .Renviron file. approach ensures environment variable persists across R sessions Shiny app runs background. set commands open .Renviron file modification: Add following line .Renviron, replacing “APIKEY” actual API key: OPENAI_API_KEY=“APIKEY”. need restart R session changes take effect. can clicking Session menu RStudio selecting Restart R. Caution: ’re using version control systems like GitHub GitLab, remember include .Renviron .gitignore file prevent exposing API key! maintain privacy data using gptstudio, highlight, include prompt, otherwise upload sensitive data, code, text remain confidential. Now ready use openai models quallmer package! example, can test setup running example sentiment analysis.","code":"require(usethis) edit_r_environ()"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"loading-packages-and-data-and-defining-a-task","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Loading packages and data and defining a task","title":"The quallmer trail","text":"start loading necessary packages sample dataset. define custom task want LLMs perform.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Loading packages and data and defining a task","what":"Loading packages and data","title":"The quallmer trail","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60] # turn corpus into data frame data_corpus_inaugural <- quanteda::convert(data_corpus_inaugural, to = \"data.frame\")"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"defining-a-custom-prompt","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Loading packages and data and defining a task","what":"Defining a custom prompt","title":"The quallmer trail","text":"similar defining custom prompt annotate() function. , define prompt instructs LLM score documents based alignment political left.","code":"prompt <- \"Score the following document on a scale of how much it aligns with the political left. The political left is defined as groups which advocate for social equality, government intervention in the economy, and progressive policies. Use the following metrics: SCORING METRIC: 3 : extremely left 2 : very left 1 : slightly left 0 : not at all left\""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"defining-the-structure-of-the-response-with-define_task","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Loading packages and data and defining a task","what":"Defining the structure of the response with define_task()","title":"The quallmer trail","text":"task() function allows us specify expected structure LLM’s response. following important arguments users need specify: name: descriptive name task. system_prompt: prompt guides LLM perform task. type_def: Defines expected structure response using ellmers type specifications type_object(), type_array(), etc. information use ellmer’s type specifications, please refer ellmer documentation type specifications.","code":"# Define the custom task using task() ideology_scores <- task(   name = \"Score Political Left Alignment\",   system_prompt = prompt,   type_def = type_object(     score = type_number(\"Score\"),     explanation = type_string(\"Explanation\")   ),   input_type = \"text\" )"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"define-different-trail-settings","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Define different trail settings","title":"The quallmer trail","text":"defined task, can now set different trail settings compare different LLM configurations affect results. , can use trail_setting() function. example, create four settings different models different temperature values see affect LLM’s responses.","code":"setting_gptmini0 <- trail_setting( provider = \"openai\", model = \"gpt-4.1-mini\", temperature = 0 )  setting_gptmini7 <- trail_setting( provider = \"openai\", model = \"gpt-4.1-mini\", temperature = 0.7 )  setting_gpt400 <- trail_setting( provider = \"openai\", model = \"gpt-4o\", temperature = 0 )  setting_gpt407 <- trail_setting( provider = \"openai\", model = \"gpt-4o\", temperature = 0.7 )"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"record-a-single-trail-with-a-specific-setting","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Record a single trail with a specific setting","title":"The quallmer trail","text":"can use trail_record() function record single trail specific setting. useful ensuring reproducibility results given configuration. example shows record metadata trail using setting_T0 defined earlier. result function data frame containing LLM-generated annotations document dataset well associated metadata setting used, task, data, timestamp, etc. can save output future reference ensure can reproduce results later. can also share output others allow verify findings.","code":"rec_T0 <- trail_record( data = data_corpus_inaugural, text_col = \"text\", task = ideology_scores, setting = setting_gptmini0, id_col = \"doc_id\" ) ## Running task 'Score Political Left Alignment' using model: gpt-4.1-mini ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% # Display the recorded trail's settings library(dplyr) ##  ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ##  ##     filter, lag ## The following objects are masked from 'package:base': ##  ##     intersect, setdiff, setequal, union library(kableExtra) ##  ## Attaching package: 'kableExtra' ## The following object is masked from 'package:dplyr': ##  ##     group_rows meta_df <- tibble(   field = names(rec_T0$meta),   value = vapply(rec_T0$meta, function(x) {     if (is.list(x)) {       # pretty-print lists       paste(capture.output(str(x, max.level = 1)), collapse = \"<br>\")     } else if (length(x) > 1) {       paste(x, collapse = \", \")     } else {       as.character(x)     }   }, FUN.VALUE = character(1)) )  meta_df %>%   kable(\"html\", escape = FALSE, col.names = c(\"Field\", \"Value\")) %>%   kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %>%   column_spec(1, bold = TRUE)"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"run-multiple-trails-with-different-settings","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Run multiple trails with different settings","title":"The quallmer trail","text":"step involves running task data across multiple settings using trail_compare() function. allows us see different configurations impact LLM’s outputs.","code":"left_trails <- trail_compare( data = data_corpus_inaugural, text_col = \"text\", task = ideology_scores, settings = list(   T0 = setting_gptmini0,   T07 = setting_gptmini7,   T40 = setting_gpt400,   T407 = setting_gpt407 ), id_col = \"doc_id\" ) ## Running task 'Score Political Left Alignment' using model: gpt-4.1-mini ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% ## Running task 'Score Political Left Alignment' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% ## Running task 'Score Political Left Alignment' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"build-the-output-by-trail-matrix","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Build the output-by-trail matrix","title":"The quallmer trail","text":"step combines outputs different trails matrix format, row represents document column represents output specific trail.","code":"trail_mat <- trail_matrix( x = left_trails, id_col = \"doc_id\", label_col = \"score\" )  # Display the trail matrix  trail_mat ## # A tibble: 4 × 5 ##   doc_id        T0   T07   T40  T407 ##   <chr>      <dbl> <dbl> <dbl> <dbl> ## 1 2013-Obama     2     2     2     3 ## 2 2017-Trump     0     0     0     0 ## 3 2021-Biden     1     1     2     1 ## 4 2025-Trump     0     0     0     0"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"compute-agreement-across-trails","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Compute agreement across trails","title":"The quallmer trail","text":"step assesses stability reliability LLM-generated annotations across different trails using intercoder reliability metrics. output shows intercoder reliability metrics across different trails, indicating consistent LLM-generated annotations across various settings. Higher values suggest greater reliability stability annotations. Overall, trail functionality quallmer package provides systematic way assess reproducibility reliability LLM-generated annotations recording metadata single LLM runs comparing results across multiple runs different configurations. particularly useful researchers want ensure findings robust overly dependent specific LLM settings. also helps decision model settings use given annotation task.","code":"trail_icr <- trail_agreement( x = left_trails, id_col = \"doc_id\", label_col = \"score\" )  trail_icr ##                            metric  value ## 1                  units_included 4.0000 ## 2                          coders 4.0000 ## 3                      categories 4.0000 ## 4         percent_unanimous_units 0.5000 ## 5 mean_pairwise_percent_agreement 0.7500 ## 6      mean_pairwise_cohens_kappa 0.6343 ## 7             kripp_alpha_nominal 0.6225 ## 8                    fleiss_kappa 0.6145"},{"path":"https://seraphinem.github.io/quallmer/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Seraphine F. Maerz. Author, maintainer. Kenneth Benoit. Author.","code":""},{"path":"https://seraphinem.github.io/quallmer/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Maerz S, Benoit K (2025). quallmer: Qualitative analysis large language models. R package version 0.1.0, https://SeraphineM.github.io/quallmer.","code":"@Manual{,   title = {quallmer: Qualitative analysis with large language models},   author = {Seraphine F. Maerz and Kenneth Benoit},   year = {2025},   note = {R package version 0.1.0},   url = {https://SeraphineM.github.io/quallmer}, }"},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"quallmer-","dir":"","previous_headings":"","what":"Qualitative analysis with large language models","title":"Qualitative analysis with large language models","text":"quallmer package easy--use toolbox qualitative researchers quickly apply AI-assisted annotation texts, images, pdfs, tabular data structured data. Using annotate(), users can generate structured, interpretable outputs powered large language models (LLMs). package includes library predefined tasks common qualitative coding needs, sentiment analysis, thematic coding, stance detection. also allows users create custom annotation tasks tailored specific research questions data types using task(). ensure quality reliability AI-generated annotations, quallmer offers tools comparing LLM outputs human-coded data assessing inter-coder reliability. agreement(), users can launch interactive app manually code data, review AI annotations, evaluate intercoder reliability coders agreement LLM-generated scores. quallmer** package makes AI-assisted qualitative coding accessible without requiring deep expertise R, programming machine learning.**","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"core-functions","dir":"","previous_headings":"","what":"Core functions","title":"Qualitative analysis with large language models","text":"package provides following core functions:","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"annotate","dir":"","previous_headings":"","what":"annotate()","title":"Qualitative analysis with large language models","text":"generic function works LLM supported ellmer. Generates structured responses based predefined user-defined tasks.","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"task","dir":"","previous_headings":"","what":"task()","title":"Qualitative analysis with large language models","text":"Creates custom annotation tasks tailored specific research questions data types. Uses system_prompt various type specifications ellmer package define LLM interpret inputs format outputs. Tasks created task() can passed directly annotate(). allows users tailor annotation process specific data types makes package extensible future use cases.","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"agreement_app","dir":"","previous_headings":"","what":"agreement_app()","title":"Qualitative analysis with large language models","text":"Manually code data Review validate LLM-generated annotations Compare human-coded data LLM-generated annotations evaluate agreement inter-coder reliability various metrics (e.g., Krippendorff’s alpha, Cohen’s Fleiss’ kappa).","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"agreement","dir":"","previous_headings":"","what":"agreement()","title":"Qualitative analysis with large language models","text":"Calculates intercoder reliability scores (e.g., Krippendorff’s alpha Cohen’s Fleiss’ kappa) multiple human coders human coders LLM-generated annotations. Works similar Agreement App can used programmatically without launching app.","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"the-quallmer-trail-","dir":"","previous_headings":"","what":"The quallmer trail","title":"Qualitative analysis with large language models","text":"Apart core functions , quallmer package also provides set functions ensure reproducibility reliability LLM-generated annotations systematic comparisons across multiple LLM runs different settings. “trail” functionality adds reproducibility layer top annotate() following workflow: Define trail settings Describe LLM trail, .e., LLMs called (e.g., model, temperature). trail_setting() ↓ Record single LLM trail Record single LLM runs given task specific setting (good reproducibility). trail_record(data, text_col, task, setting) ↓ Run multiple trails different settings Run task data across multiple settings (e.g., different LLMs, different temperatures). trail_compare(data, text_col, task, settings = list(...)) ↓ Build output––trail matrix Treat trail unique “output” combine outputs. trail_matrix(trail_compare_obj) ↓ Compute agreement across trails Assess stability / reliability across multiple LLM runs task different settings. trail_agreement(trail_compare_obj)","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"supported-llms","dir":"","previous_headings":"","what":"Supported LLMs","title":"Qualitative analysis with large language models","text":"package supports LLMs currently available ellmer package. authentication usage LLMs, please refer respective ellmer documentation see tutorial setting openai API key getting started open-source Ollama model.","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Qualitative analysis with large language models","text":"can install development version quallmer https://github.com/SeraphineM/quallmer :","code":"# install.packages(\"pak\") pak::pak(\"SeraphineM/quallmer\")"},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"example-use-and-tutorials","dir":"","previous_headings":"","what":"Example use and tutorials","title":"Qualitative analysis with large language models","text":"learn use package, please refer step--step tutorials illustrations use predefined tasks.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute intercoder reliability statistics — agreement","title":"Compute intercoder reliability statistics — agreement","text":"function computes set intercoder reliability statistics nominal coding data multiple coders: Krippendorff's alpha (nominal), Fleiss' kappa, mean pairwise Cohen's kappa, mean pairwise percent agreement, share unanimous units, basic counts.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute intercoder reliability statistics — agreement","text":"","code":"agreement(data, unit_id_col, coder_cols, min_coders = 2L)"},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute intercoder reliability statistics — agreement","text":"data data frame containing unit identifier coder columns. unit_id_col Character scalar. Name column identifying units (e.g. document ID, paragraph ID). coder_cols Character vector. Names columns containing coders' codes (column = one coder). min_coders Integer: minimum number non-missing coders per unit unit included. Default 2.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute intercoder reliability statistics — agreement","text":"data frame two columns: metric Name statistic value Estimated value","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute intercoder reliability statistics — agreement","text":"","code":"if (FALSE) { # \\dontrun{ agreement(   data = my_df,   unit_id_col = \"doc_id\",   coder_cols  = c(\"coder1\", \"coder2\", \"coder3\") ) } # }"},{"path":"https://seraphinem.github.io/quallmer/reference/agreement_app.html","id":null,"dir":"Reference","previous_headings":"","what":"Launch the Agreement App — agreement_app","title":"Launch the Agreement App — agreement_app","text":"Starts Shiny app manual coding, LLM checking, agreement calculation. - LLM mode, can also select metadata columns. - Agreement mode, select unit ID coder columns (text column).","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement_app.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Launch the Agreement App — agreement_app","text":"","code":"agreement_app()"},{"path":"https://seraphinem.github.io/quallmer/reference/agreement_app.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Launch the Agreement App — agreement_app","text":"shiny.appobj","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/annotate.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply an annotation task to input data — annotate","title":"Apply an annotation task to input data — annotate","text":"Automatically detects correct task type (e.g., text, image). Delegates actual processing task's internal run() method.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/annotate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply an annotation task to input data — annotate","text":"","code":"annotate(.data, task, ...)"},{"path":"https://seraphinem.github.io/quallmer/reference/annotate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply an annotation task to input data — annotate","text":".data Input data (text, image, etc.) task task created [task()] ... Additional arguments passed task$run()","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/annotate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply an annotation task to input data — annotate","text":"Structured data frame results","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_LMRDsample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample from Large Movie Review Dataset (Maas et al. 2011) — data_corpus_LMRDsample","title":"Sample from Large Movie Review Dataset (Maas et al. 2011) — data_corpus_LMRDsample","text":"sample 100 positive 100 negative reviews Maas et al. (2011) dataset sentiment classification.  original dataset contains 50,000 highly polar movie reviews.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_LMRDsample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample from Large Movie Review Dataset (Maas et al. 2011) — data_corpus_LMRDsample","text":"","code":"data_corpus_LMRDsample"},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_LMRDsample.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample from Large Movie Review Dataset (Maas et al. 2011) — data_corpus_LMRDsample","text":"corpus docvars consist : docnumber serial (within set polarity) document number rating user-assigned movie rating 1-10 point integer scale polarity either `neg` `pos` indicate whether     movie review negative positive.  See Maas et al (2011)     cut-values governed assignment.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_LMRDsample.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Sample from Large Movie Review Dataset (Maas et al. 2011) — data_corpus_LMRDsample","text":"<http://ai.stanford.edu/~amaas/data/sentiment/>","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_LMRDsample.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sample from Large Movie Review Dataset (Maas et al. 2011) — data_corpus_LMRDsample","text":"Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew   Y. Ng, Christopher Potts. (2011). \"[Learning Word Vectors Sentiment   Analysis](http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf)\".   49th Annual Meeting Association Computational Linguistics (ACL   2011).","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_LMRDsample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample from Large Movie Review Dataset (Maas et al. 2011) — data_corpus_LMRDsample","text":"","code":"if (FALSE) { # \\dontrun{ library(quanteda)  # define a sentiment task task_posneg <- task(   name = \"Sentiment analysis of movie reviews\",   system_prompt = \"You will rate the sentiment from movie reviews.\",   type_def = type_object(     polarity_llm = type_enum(c(\"pos\", \"neg\"),     description = \"Sentiment label (pos = positive, neg = negative\")   ) )  set.seed(10001) test_corpus <- data_corpus_LMRDsample %>%   corpus_sample(size = 10, by = polarity)  result <- test_corpus %>%   annotate(task_posneg, chat_fn = chat_openai, model = \"gpt-4.1-mini\") %>%   cbind(data.frame(polarity_human = test_corpus$polarity))  agreement(result, \"id\", coder_cols = c(\"polarity_llm\", \"polarity_human\")) } # }"},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_manifsentsUK2010sample.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample of UK manifesto sentences 2010 crowd-annotated for immigration — data_corpus_manifsentsUK2010sample","title":"Sample of UK manifesto sentences 2010 crowd-annotated for immigration — data_corpus_manifsentsUK2010sample","text":"corpus sentences sampled publicly available party   manifestos United Kingdom 2010 election.  sentence   rated terms classification pertaining immigration   scale favorability toward open immigration   policy (mean score crowd coders scale -1 (favours open   immigration policy), 0 (neutral), 1 (anti-immigration). sentences sampled corpus used [Benoit et al.   (2016)](https://doi.org/10.1017/S0003055416000058), contains   information crowd-sourced annotation  approach.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_manifsentsUK2010sample.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample of UK manifesto sentences 2010 crowd-annotated for immigration — data_corpus_manifsentsUK2010sample","text":"","code":"data_corpus_manifsentsUK2010sample"},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_manifsentsUK2010sample.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Sample of UK manifesto sentences 2010 crowd-annotated for immigration — data_corpus_manifsentsUK2010sample","text":"corpus consists 155 sentences randomly sampled party manifestos, attempt balance   sentencs according categorisation pertaining immigration , well party.   corpus contains following document-level variables: party factor; abbreviation party wrote manifesto. partyname factor; party wrote manifesto. year integer; 4-digit year election. crowd_immigration_label Factor indicating whether majority   crowd workers labelled sentence referring immigration .   variable missing values (`NA`) non-annotated manifestos. crowd_immigration_mean numeric; direction   statements coded \"Immigration\" based aggregated crowd codings.   variable mean scores assigned workers coded   sentence allocated sentence \"Immigration\" category.   variable ranges -1 (Favorable open immigration policy) +1   (\"Negative closed immigration policy\"). crowd_immigration_n integer; number coders   contributed   mean score `crowd_immigration_mean`. [corpus][quanteda::corpus] object.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_manifsentsUK2010sample.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Sample of UK manifesto sentences 2010 crowd-annotated for immigration — data_corpus_manifsentsUK2010sample","text":"Benoit, K., Conway, D., Lauderdale, B.E., Laver, M., & Mikhaylov, S. (2016).   [Crowd-sourced Text Analysis:   Reproducible Agile Production Political Data](https://doi.org/10.1017/S0003055416000058).   *American Political Science Review*, 100,(2), 278–295.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/data_corpus_manifsentsUK2010sample.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Sample of UK manifesto sentences 2010 crowd-annotated for immigration — data_corpus_manifsentsUK2010sample","text":"","code":"if (FALSE) { # \\dontrun{ library(quanteda)  immigration_instructions <- \"This task involves reading sentences from political texts from the 2010 UK general election, and judging whether these statements deal with immigration policy. Each sentence may or may not be related to immigration policy.  First, you will read a short section from a party manifesto. For the sentence highlighted in red, enter your best judgment about whether it refers to some aspect of immigration policy, or not. Most sentences will not relate to immigration policy - it is your job to find and rate those that do. If the sentence does not refer to immigration policy, you should select 'Not immigration policy' and proceed directly to the next sentence. If the sentence does refer to immigration policy, you should indicate this by checking this option.  If you indicate that the sentence is immigration policy, you will be asked to give your best judgment of the policy position on immigration being expressed in the sentence. This will range from a very open and favourable position on immigration, to a very closed and negative stance on immigration. These are coded on a five-point scale, with a neutral position (neither favouring nor opposing) immigration lying in the middle.\"  immigration_description <- \"What is 'immigration policy'?  Immigration policy relates to all government policies, laws, regulations, and practices that deal with the free travel of foreign persons across the country's borders, especially those that intend to live, work, or seek legal protection (asylum) in that country. Examples of specific policies that pertain to immigration include the regulation of: work permits for foreign nationals; residency permits for foreign nationals; asylum seekers and their treatment; requirements for acquiring citizenship; illegal immigrants and migrant workers (and their families) living or working illegally in the country.  It also includes favorable or unfavorable general statements about immigrants or immigration policy, such as statements indicating that immigration has been good for a country, or that immigrants have forced local people out of jobs, etc.\"  immigration_scale <- \"Pro-immigration policies (a value of -1)  Examples of 'pro' immigration positions include: Positive statements about the benefits of immigration, such as economic or cultural benefits; Statements about the moral obligation to welcome asylum seekers; Policies that would improve conditions for asylum seekers and their families; Urging an increase the number of work permits for foreign nationals; Making it possible for illegal immigrants to obtain a legal status or even citizenship; Reducing barriers to immigration generally.  Anti-immigration policies (a value of 1)  Examples of 'anti' immigration positions include: Negative statements about consequences of immigration, such as job losses, increased crime, or destruction of national culture; Arguments about asylum seekers abusing the system; Policies to deport asylum seekers and their families; Urging restrictions on the number of work permits for foreign nationals, including points systems; Deporting illegal immigrants and their families; Increasing barriers to immigration generally.  Neutral immigration policy statements (a value of 0)  Examples of neutral statements about immigration policy: Advocating a balanced approach to the problem; Statements about administrative capacity for handling immigration or asylum seekers; Statements that do not take a pro- or anti-immigration stance generally, despite making some statement about immigration.\"  # define a sentiment task task_immigration <- task(   name = \"Immigration policy\",   system_prompt = immigration_instructions,   type_def = type_object(     llm_immigration_label = type_enum(c(\"Immigration\", \"Not immmigration\")),     llm_immigration_scale = type_integer(immigration_scale),   ) )  result <- data_corpus_manifsentsUK2010sample %>%   annotate(task_immigration,            chat_fn = chat_google_gemini, model = \"gemini-2.5-flash\")  result_combined <- cbind(   result,   data.frame(crowd_immigration_label = data_corpus_manifsentsUK2010sample$crowd_immigration_label) )  # compute agreement agreement(result_combined, unit_id_col = \"id\",           coder_cols = c(\"llm_immigration_label\", \"crowd_immigration_label\"))  # confusion matrix table(tmp$llm_immigration_label, tmp$crowd_immigration_label) } # }"},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for trail_compare — print.trail_compare","title":"Print method for trail_compare — print.trail_compare","text":"Print method trail_compare","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for trail_compare — print.trail_compare","text":"","code":"# S3 method for class 'trail_compare' print(x, ...)"},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for trail_compare — print.trail_compare","text":"x trail_compare object. ... Ignored.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_setting.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for trail_setting — print.trail_setting","title":"Print method for trail_setting — print.trail_setting","text":"Print method trail_setting","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_setting.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for trail_setting — print.trail_setting","text":"","code":"# S3 method for class 'trail_setting' print(x, ...)"},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_setting.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for trail_setting — print.trail_setting","text":"x trail_setting object. ... Ignored.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/quallmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"quallmer: Qualitative analysis with large language models — quallmer-package","title":"quallmer: Qualitative analysis with large language models — quallmer-package","text":"Provides userfriendly support qualitative analysis large language models (LLMs).","code":""},{"path":[]},{"path":"https://seraphinem.github.io/quallmer/reference/quallmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"quallmer: Qualitative analysis with large language models — quallmer-package","text":"Maintainer: Seraphine F. Maerz seraphine.maerz@unimelb.edu.au (ORCID) Authors: Kenneth Benoit kbenoit@smu.edu.sg (ORCID)","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task.html","id":null,"dir":"Reference","previous_headings":"","what":"Define an annotation task — task","title":"Define an annotation task — task","text":"flexible task definition wrapper ellmer. Supports structured output type, including `type_object()`, `type_array()`, `type_enum()`, `type_boolean()`, others.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define an annotation task — task","text":"","code":"task(name, system_prompt, type_def, input_type = \"text\")"},{"path":"https://seraphinem.github.io/quallmer/reference/task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define an annotation task — task","text":"name Name task. system_prompt System prompt guide model (required ellmer's `chat_fn`). type_def Structured output definition, e.g., created `ellmer::type_object()`, `ellmer::type_array()`, `ellmer::type_enum()`. input_type Type input data: `\"text\"`, `\"image\"`, `\"audio\"`, etc.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define an annotation task — task","text":"task object `run()` method.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_fact.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for overall truthfulness assessment — task_fact","title":"Predefined task for overall truthfulness assessment — task_fact","text":"Assigns overall truthfulness score text lists topics reduce confidence accuracy.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_fact.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for overall truthfulness assessment — task_fact","text":"","code":"task_fact(max_topics = 5)"},{"path":"https://seraphinem.github.io/quallmer/reference/task_fact.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predefined task for overall truthfulness assessment — task_fact","text":"max_topics Integer: maximum number topics issues list reducing confidence truthfulness text. Default 5.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_fact.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for overall truthfulness assessment — task_fact","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_ideology.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for ideological scaling on a specified dimension — task_ideology","title":"Predefined task for ideological scaling on a specified dimension — task_ideology","text":"Ideological scaling specified dimension, justification.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_ideology.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for ideological scaling on a specified dimension — task_ideology","text":"","code":"task_ideology(   dimension = \"the specified ideological dimension (0 = first pole, 10 = second pole)\",   definition = NULL )"},{"path":"https://seraphinem.github.io/quallmer/reference/task_ideology.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predefined task for ideological scaling on a specified dimension — task_ideology","text":"dimension character string specifying ideological dimension, ideally naming poles, e.g., \"liberal - illiberal\", \"left - right\", \"inclusive - exclusive\". first pole corresponds 0 second 10. definition Optional detailed explanation dimension means. provided, included system prompt guide annotation.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_ideology.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for ideological scaling on a specified dimension — task_ideology","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_salience.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for salience of topics discussed (ranked topics) — task_salience","title":"Predefined task for salience of topics discussed (ranked topics) — task_salience","text":"Ranked list topics mentioned text, ordered salience.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_salience.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for salience of topics discussed (ranked topics) — task_salience","text":"","code":"task_salience(topics = NULL, max_topics = 5)"},{"path":"https://seraphinem.github.io/quallmer/reference/task_salience.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predefined task for salience of topics discussed (ranked topics) — task_salience","text":"topics Optional character vector predefined topic labels (e.g., c(\"economy\", \"health\", \"education\", \"environment\")). supplied, model classify rank among topics. NULL, model may infer topic labels directly text. max_topics Integer: maximum number topics return topics inferred text. Default 5.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_salience.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for salience of topics discussed (ranked topics) — task_salience","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_sentiment.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for sentiment analysis — task_sentiment","title":"Predefined task for sentiment analysis — task_sentiment","text":"Predefined task sentiment analysis","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_sentiment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for sentiment analysis — task_sentiment","text":"","code":"task_sentiment()"},{"path":"https://seraphinem.github.io/quallmer/reference/task_sentiment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for sentiment analysis — task_sentiment","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_stance.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for stance detection (position taking) — task_stance","title":"Predefined task for stance detection (position taking) — task_stance","text":"Predefined task stance detection (position taking)","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_stance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for stance detection (position taking) — task_stance","text":"","code":"task_stance(topic = \"the given topic\")"},{"path":"https://seraphinem.github.io/quallmer/reference/task_stance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predefined task for stance detection (position taking) — task_stance","text":"topic character string specifying topic stance detection.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_stance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for stance detection (position taking) — task_stance","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_agreement.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute agreement across Trail settings — trail_agreement","title":"Compute agreement across Trail settings — trail_agreement","text":"Convenience helper compute intercoder reliability across multiple Trail records Trail comparison treating setting coder.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_agreement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute agreement across Trail settings — trail_agreement","text":"","code":"trail_agreement(   x,   id_col = \"id\",   label_col = \"label\",   min_coders = 2L,   agreement_fun = agreement )"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_agreement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute agreement across Trail settings — trail_agreement","text":"x trail_compare object list trail_record objects. id_col Character scalar. Name unit identifier column resulting wide data (defaults \"id\"). label_col Character scalar. Name label column record's annotations (defaults \"label\"). min_coders Integer. Minimum number non-missing coders per unit required inclusion. Passed agreement_fun. agreement_fun Function used compute agreement. Defaults agreement(), expected accept data, unit_id_col, coder_cols, min_coders.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_agreement.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute agreement across Trail settings — trail_agreement","text":"result calling agreement_fun() wide data,   typically data frame agreement statistics.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Trail compare: run the same task across multiple settings — trail_compare","title":"Trail compare: run the same task across multiple settings — trail_compare","text":"Apply quallmer task data text column set settings, returning one trail_record per setting.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trail compare: run the same task across multiple settings — trail_compare","text":"","code":"trail_compare(   data,   text_col,   task,   settings,   id_col = NULL,   cache_dir = \"trail_cache\",   overwrite = FALSE,   annotate_fun = annotate )"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trail compare: run the same task across multiple settings — trail_compare","text":"data data frame containing text annotated. text_col Character scalar. Name text column. task quallmer task object. settings named list trail_setting objects. names used identifiers setting (e.g. coder IDs). id_col Optional character scalar. Name unit identifier column. NULL, temporary \".trail_unit_id\" created shared across records. cache_dir Optional directory caching. Passed trail_record(). overwrite Logical. TRUE, ignore cache settings recompute. annotate_fun Function used perform annotation, passed trail_record().","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_compare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trail compare: run the same task across multiple settings — trail_compare","text":"object class \"trail_compare\" containing named   list trail_record objects basic metadata.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Trail records to coder-style wide data — trail_matrix","title":"Convert Trail records to coder-style wide data — trail_matrix","text":"Treat setting/record Trail comparison separate coder convert annotations wide data frame suitable agreement analysis.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Trail records to coder-style wide data — trail_matrix","text":"","code":"trail_matrix(x, id_col = \"id\", label_col = \"label\")"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Trail records to coder-style wide data — trail_matrix","text":"x Either trail_compare object named list trail_record objects. id_col Character scalar. Name column identifies units (documents, paragraphs, etc.). Must present record's annotations data. label_col Character scalar. Name column record's annotations data containing code label interest.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Trail records to coder-style wide data — trail_matrix","text":"data frame one row per unit one column per   setting/record. unit ID column retained name   id_col.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_record.html","id":null,"dir":"Reference","previous_headings":"","what":"Trail record: reproducible quallmer annotation — trail_record","title":"Trail record: reproducible quallmer annotation — trail_record","text":"Run quallmer task data frame specified LLM setting, capturing metadata reproducibility optionally caching full result disk.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_record.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trail record: reproducible quallmer annotation — trail_record","text":"","code":"trail_record(   data,   text_col,   task,   setting,   id_col = NULL,   cache_dir = \"trail_cache\",   overwrite = FALSE,   annotate_fun = annotate )"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_record.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trail record: reproducible quallmer annotation — trail_record","text":"data data frame containing text annotated. text_col Character scalar. Name text column. task quallmer task object. setting trail_setting object describing LLM configuration. id_col Optional character scalar identifying units. cache_dir Optional directory cache Trails. NULL, caching disabled. overwrite Whether overwrite existing cache. annotate_fun Function used perform annotation (default annotate()).","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_record.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trail record: reproducible quallmer annotation — trail_record","text":"object class \"trail_record\".","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_setting.html","id":null,"dir":"Reference","previous_headings":"","what":"Trail setting specification — trail_setting","title":"Trail setting specification — trail_setting","text":"Define reproducible specification LLM setting use quallmer Trail. object captures provider, model name, temperature, optional extra arguments.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_setting.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trail setting specification — trail_setting","text":"","code":"trail_setting(   provider = \"openai\",   model = \"gpt-4.1-mini\",   temperature = 0,   extra = list() )"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_setting.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trail setting specification — trail_setting","text":"provider Character. Backend provider, e.g. \"openai\", \"ollama\", \"azure\". model Character. Model identifier, e.g. \"gpt-4.1-mini\", \"gpt-4o-mini\", \"llama3.1:8b\". temperature Numeric scalar. Sampling temperature (default 0). extra Named list additional model arguments passed `annotate()` via `api_args` needed (e.g. penalties safety flags).","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_setting.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trail setting specification — trail_setting","text":"object class \"trail_setting\".","code":""}]
