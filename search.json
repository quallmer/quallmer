[{"path":"https://seraphinem.github.io/quallmer/articles/getting-started.html","id":"basic-usage","dir":"Articles","previous_headings":"","what":"Basic usage","title":"Getting started with quallmer","text":"quallmer package developed using R. Please make sure recent version R RStudio installed computer. new R RStudio, can find great free--charge 1.5h introduction R RStudio instats. get started quallmer, first need install package GitHub. , can load package begin using functions.","code":"# If you don't have pak installed yet, uncomment and run the following line: # install.packages(\"pak\") # Then, install quallmer using pak: pak::pak(\"SeraphineM/quallmer\") library(quallmer) #> Loading required package: ellmer"},{"path":"https://seraphinem.github.io/quallmer/articles/getting-started.html","id":"overview-of-tutorials","dir":"Articles","previous_headings":"","what":"Overview of tutorials","title":"Getting started with quallmer","text":"using large language models, users need sign API key LLM provider, openai, download open-source model like Ollama. quallmer package supports multiple LLM providers ellmer package, allowing users choose one best fits needs. Using annotate(), users can generate structured, interpretable outputs powered large language models (LLMs). package includes library predefined tasks common qualitative coding needs, sentiment analysis, thematic coding, stance detection. also allows users create custom annotation tasks tailored specific research questions data types using task(). tutorials guide following topics: Signing OpenAI API key: tutorial guide process obtaining API key OpenAI, necessary using OpenAI’s LLMs quallmer package. Working open-source Ollama model: tutorial demonstrate use quallmer package open-source Ollama model qualitative coding tasks. Using predefined tasks: illustrations show utilize library predefined annotation tasks available quallmer package perform common qualitative coding tasks efficiently. Creating custom tasks: tutorial walk process defining custom annotation tasks using task() function, allowing tailor LLM’s output specific research needs. Using Agreement App: tutorial introduce Agreement App manually code data, check LLM annotations, calculate interrater reliability user-friendly intuitive interface. hope tutorials help get started quallmer package empower leverage large language models qualitative research projects!","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/overview.html","id":"predefined-tasks","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Predefined tasks","title":"Overview of predefined tasks","text":"quallmer package currently includes following predefined tasks: wish create custom tasks tailored specific research questions data types, can use task() function. function allows specify system prompt output structure, enabling customize annotation process according needs. tutorial create custom tasks, please refer custom tasks tutorial.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_fact.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Fact checking of claims","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_fact.html","id":"using-annotate-for-fact-checking-of-claims-in-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for fact checking of claims in texts","title":"Example: Fact checking of claims","text":"text political speech contains several claims promises subjective speculative. Transfer power people: claim power transferred Washington people common rhetorical device lacks specific mechanisms evidence. American carnage: depiction U.S. suffering widespread decay crime exaggerated universally supported data. Redistribution wealth: assertion wealth redistributed globally expense American middle class complex economic issue oversimplified. Protectionism benefits: idea protectionist policies lead prosperity debated among economists guaranteed outcome. Eradication terrorism: promise eradicate radical Islamic terrorism ambitious goal difficult achieve longstanding challenge many administrations.","code":"# Apply predefined fact checking task with task_fact() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_fact(),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Fact-checking' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 1 -> 3 | ■■■■■■■■■■■■■■■■■■■■■■■           75% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_fact.html","id":"using-annotate-for-fact-checking-with-a-specific-number-of-claims-to-check","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for fact checking with a specific number of claims to check","title":"Example: Fact checking of claims","text":"speech contains several broad sweeping claims rhetorical factual. Transfer Power People: claim power transferred Washington people common political rhetoric lacks specific evidence mechanisms speech. American Carnage: description state nation ‘American carnage’ dramatic portrayal may accurately reflect overall condition country time. America First Policy: ‘America First’ policy stated goal, implications outcomes policies complex guaranteed result benefits claimed. text contains several misleading inaccurate claims. Historical inaccuracies: statement Panama Canal control China misleading. canal operated Panama Canal Authority, China. Policy claims: Many policy promises, ending Green New Deal changing Gulf Mexico’s name, either exaggerated feasible without significant legislative action. Election results: Claims winning popular vote millions unifying support across demographics lack evidence likely exaggerated. example, demonstrated use annotate() function task_fact() fact-check claims corpus innaugural speeches. results include truth score, identified misleading topics, explanations claim evaluated. amount claims check can adjusted using max_topics parameter task_fact() function. Now can apply approach texts fact-checking purposes!","code":"# Apply predefined fact checking task with task_fact() in the annotate() function result_claims <- annotate(data_corpus_inaugural, task = task_fact(max_topics = 3),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Fact-checking' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_ideology.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Ideology detection","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_ideology.html","id":"using-annotate-for-ideological-scaling-of-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for ideological scaling of texts","title":"Example: Ideology detection","text":"","code":"# Define ideological dimension dimension <- \"inclusive–exclusive\" # Provide definition for the dimension definition <- \"Inclusive language emphasizes equal rights, diversity, pluralism,  and protection of minorities, whereas exclusive language emphasizes exclusion  of groups, national homogeneity, and restricting rights.\" # Apply predefined ideology task with task_ideology() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_ideology(dimension, definition),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Ideological scaling' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_ideology.html","id":"adjusting-the-ideology-scaling-task","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Adjusting the ideology scaling task","title":"Example: Ideology detection","text":"can customize ideological scaling task defining task task() (detailed explanation, see “Defining custom tasks” tutorial). example, might like change scale 0-10 -5 +5. example, demonstrated use task_ideology() scaling texts regarding ideological position specified dimension. also showed customize task using task() function tailored annotation needs, e.g., changing scale 0-10 -5 +5. Now can apply techniques text data ideological analysis!","code":"custom_ideology <- task(     name = \"Ideological scaling\",     system_prompt = paste0(       \"You are an expert political scientist performing ideological text scaling.\",       \"Task:\",       \"- Read each short text carefully.\",       \"- Place the text on a -5 to +5 scale for the following ideological dimension: \",       dimension,        definition     ),     type_def = ellmer::type_object(       score       = ellmer::type_integer(\"Ideological position on the specified dimension (0–10, where -5 = first pole, +5 = second pole)\"),       explanation = ellmer::type_string(\"Brief justification for the assigned score, referring to specific elements in the text\")     ),     input_type = \"text\"   ) # Apply the custom task custom_result <- annotate(data_corpus_inaugural, task = custom_ideology,                            chat_fn = chat_openai, model = \"gpt-4o\",                           api_args = list(temperature = 0)) ## Running task 'Ideological scaling' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_salience.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Salience of topics","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_salience.html","id":"using-annotate-for-salience-of-any-topics-discussed-in-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for salience of ANY topics discussed in texts","title":"Example: Salience of topics","text":"American values principles: speech frequently references foundational American ideals equality, liberty, pursuit happiness, emphasizing enduring importance. Collective action unity: strong emphasis working together nation address challenges, highlighting need unity collective effort. Economic social equality: text discusses importance rising middle class, equal opportunities, social safety nets, stressing economic social justice. Role government: speech addresses need government reform balance individual initiative collective responsibility. Global leadership peace: text mentions America’s role global peace, alliances, importance addressing climate change, reflecting international responsibilities. 2017-Trump Transfer Power People , America First Policy , National Unity Patriotism , Economic Rebuilding Job Creation, Critique Political Establishment Transfer Power People: speech emphasizes returning power Washington citizens, highlighting central theme. America First Policy: repeated focus prioritizing American interests trade, immigration, foreign policy underscores importance. National Unity Patriotism: call unity patriotism recurring theme, references shared values protection. Economic Rebuilding Job Creation: significant emphasis rebuilding infrastructure, creating jobs, economic prosperity. Critique Political Establishment: speech criticizes political establishment failing people, marking key point. 2021-Biden Unity , Democracy , Challenges facing America , American values ideals , Historical context legacy Unity: speech emphasizes unity central theme, repeatedly calling Americans come together overcome challenges. Democracy: Democracy highlighted precious fragile cause prevailed, references peaceful transfer power people. Challenges facing America: text discusses various challenges pandemic, racial justice, political extremism, climate change, emphasizing need address collectively. American values ideals: speech frequently references values like liberty, dignity, truth, framing common objects love define Americans. Historical context legacy: speech draws historical events figures, Civil War Dr. King, contextualize current challenges inspire action. 2025-Trump American Sovereignty Nationalism , Government Reform Efficiency , Immigration Border Security , Economic Policies Energy Independence, National Unity Patriotism American Sovereignty Nationalism: speech emphasizes reclaiming sovereignty, restoring respect, making America envy world, highlighting strong nationalist theme. Government Reform Efficiency: significant focus reforming government structures, ending corruption, restoring competence, recurring theme throughout speech. Immigration Border Security: speaker discusses declaring national emergency southern border, halting illegal entry, addressing criminal networks, making prominent topic. Economic Policies Energy Independence: speech outlines plans economic revival, energy independence, manufacturing, emphasizing importance economic strength. National Unity Patriotism: speaker frequently mentions national unity, pride, patriotism, aiming inspire collective American identity spirit. Economy: text frequently discusses economic themes, economic recovery, prosperity, importance rising middle class. emphasizes need infrastructure, fair markets, economic vitality. Environment: clear emphasis climate change sustainable energy, highlighting need lead transition preserve natural resources. Foreign Policy: text addresses America’s role global alliances, peace, democracy, emphasizing engagement support freedom worldwide. Education: need education reform equipping children necessary skills mentioned, linking economic growth future readiness. Health: Health care mentioned context reducing costs ensuring security dignity citizens, alongside references Medicare Medicaid. 2017-Trump economy , foreign policy, education , health , environment Economy: text frequently discusses economic issues jobs, factories, wealth, infrastructure, emphasizing need rebuild economy prioritize American workers. Foreign Policy: significant emphasis foreign policy, particularly regarding trade, alliances, military spending, focus ‘America first’ protecting American interests. Education: speech mentions education system, highlighting issues current state need improvement. Health: Health indirectly referenced context eradicating disease ensuring safe neighborhoods. Environment: environment directly addressed, mention infrastructure projects like roads railways, can environmental implications. 2021-Biden health , foreign policy, economy , environment , education Health: text frequently mentions pandemic, impact, need address , highlighting salience. Foreign Policy: References repairing alliances engaging world indicate focus foreign policy. Economy: loss jobs businesses due pandemic discussed, emphasizing economic concerns. Environment: mention “climate crisis” suggests environmental issues topic concern. Education: Briefly mentioned context teaching children safe schools, indicating presence lower salience. 2025-Trump foreign policy, economy , health , education , environment Foreign Policy: text frequently discusses international relations, border security, actions foreign entities, emphasizing importance America’s global standing security. Economy: Economic issues highlighted mentions inflation, energy policies, manufacturing, trade reforms, indicating focus economic revitalization. Health: speech references public health system aims address health crises, indicating importance. Education: education system criticized current state, promises reform, showing relevance. Environment: Environmental policies mentioned, particularly context energy Green New Deal, though less prominently topics.","code":"# Apply predefined salience task with task_salience() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_salience(),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Salience (ranked topics)' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 2 -> 2 | ■■■■■■■■■■■■■■■■                  50% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% # Define a list of topics to focus on topics <- c(\"economy\", \"health\", \"education\", \"environment\", \"foreign policy\") # Apply predefined salience task with task_salience() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_salience(topics),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Salience (ranked topics)' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_salience.html","id":"adjusting-the-task_salience-so-it-also-returns-the-stance-for-each-topic","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Adjusting the task_salience() so it also returns the stance for each topic","title":"Example: Salience of topics","text":"example, demonstrated use task_salience() identifying ranking topics discussed texts, without predefined list topics. Additionally, showed customize task include stance classification topic. showcases flexibility annotate() function task framework quallmer various text analysis tasks.","code":"# Customizing the task to include the stance for each topic custom_task <- task(   name = \"Salience and stance of topics\",   system_prompt = paste(     \"You are an expert analysing the content of texts.\",     \"\",     \"Task:\",     \"- Read the text carefully.\",     \"- Identify and rank the salience of the following topics: economy, health, education, environment, foreign policy.\",     \"- For each topic mentioned, assign a stance as one of the following:\",     \"  pro, neutral, or contra.\",     \"- Append the stance directly after each topic name in the form 'topic: stance'.\",     \"- Return all topic:stance entries in descending order of salience.\",     \"- Separate entries with commas when presenting them in a list.\",     \"\",     \"Do not infer information that is not in the text.\",     \"Base all evaluations solely on the language and arguments in the document.\",     \"\",     \"Output:\",     \"- `topic_stance`: a ranked list of topic labels with stance labels appended (e.g., 'economy: pro', 'health: contra').\",     \"- `explanation`: a brief justification explaining why the topics were ordered and how stance was determined.\",     sep = \"\\n\"   ),   type_def = ellmer::type_object(     topic_stance = ellmer::type_array(       ellmer::type_string(\"Topic and stance label combined (e.g., 'economy: pro'), ranked by salience.\")     ),     explanation = ellmer::type_string(       \"Brief justification for the salience ordering and stance classification.\"     )   ),   input_type = \"text\" )  # Apply the customized task in the annotate() function custom_result <- annotate(data_corpus_inaugural, task = custom_task,                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Salience and stance of topics' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_sentiment.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Sentiment analysis","text":"","code":"library(quallmer) ## Loading required package: ellmer #Example texts texts <- c( \"This is wonderful!\", \"I really dislike this approach.\", \"The results are somewhat disappointing.\", \"Absolutely fantastic work!\" )"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_sentiment.html","id":"using-annotate-for-predefined-sentiment-analysis-of-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for predefined sentiment analysis of texts","title":"Example: Sentiment analysis","text":"","code":"# Apply predefined sentiment task with task_sentiment() in the annotate() function result <- annotate(texts, task = task_sentiment(),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Sentiment analysis' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_sentiment.html","id":"adjusting-the-sentiment-task","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Adjusting the sentiment task","title":"Example: Sentiment analysis","text":"can customize sentiment analysis task defining task task() (detailed explanation, see “Defining custom tasks” tutorial). example, might want include additional field confidence level. , might want change scoring scale 5-point Likert scale. way, can easily adapt sentiment analysis task fit specific research needs!","code":"custom_sentiment <- task(   name = \"Custom sentiment analysis\",   system_prompt = \"You are an expert annotator. Rate the sentiment of each text from -1 (very negative) to 1 (very positive), briefly explain why, and provide a confidence level from 0 to 1.\",   type_def = ellmer::type_object(     score = ellmer::type_number(\"Sentiment score between -1 (very negative) and 1 (very positive)\"),     explanation = ellmer::type_string(\"Brief explanation of the rating\"),     confidence = ellmer::type_number(\"Confidence level from 0 to 1\")   ),   input_type = \"text\" ) # Apply the custom sentiment task custom_result <- annotate(texts, task = custom_sentiment,                            chat_fn = chat_openai, model = \"gpt-4o\",                           api_args = list(temperature = 0)) ## Running task 'Custom sentiment analysis' using model: gpt-4o likert_sentiment <- task(   name = \"Likert scale sentiment analysis\",   system_prompt = \"You are an expert annotator. Rate the sentiment of each text on a scale from 1 (very negative) to 5 (very positive) and briefly explain why.\",   type_def = ellmer::type_object(     score = ellmer::type_number(\"Sentiment score between 1 (very negative) and 5 (very positive)\"),     explanation = ellmer::type_string(\"Brief explanation of the rating\")   ),   input_type = \"text\" ) # Apply the Likert scale sentiment task likert_result <- annotate(texts, task = likert_sentiment,                            chat_fn = chat_openai, model = \"gpt-4o\",                           api_args = list(temperature = 0)) ## Running task 'Likert scale sentiment analysis' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_stance.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Loading packages and data","title":"Example: Stance detection","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_stance.html","id":"using-annotate-for-stance-detection-of-texts","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Using annotate() for stance detection of texts","title":"Example: Stance detection","text":"","code":"# Define topic of interest topic <- \"Climate Change\" # Apply predefined stance task with task_stance() in the annotate() function result <- annotate(data_corpus_inaugural, task = task_stance(topic),                     chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Stance detection' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/examples/task_stance.html","id":"adjusting-the-stance-detection-task","dir":"Articles > Pkgdown > Examples","previous_headings":"","what":"Adjusting the stance detection task","title":"Example: Stance detection","text":"can customize stance detection task defining task task() (detailed explanation, see “Defining custom tasks” tutorial). example, might want include additional field confidence level. , might want LLM extract specific arguments supporting stance. Acknowledges threat climate change need respond protect future generations. Emphasizes importance leading transition sustainable energy maintain economic vitality. Recognizes scientific consensus climate change impact environmental disasters. speech emphasizes economic growth rebuilding American infrastructure without mentioning environmental considerations. focus national pride economic policies like ‘America First,’ directly relate climate change. reference environmental protection climate change initiatives. speech identifies climate change one major challenges facing nation, alongside significant issues like pandemic systemic racism. calls unity boldness addressing challenges, implying proactive stance climate action. mention ‘cry survival comes planet ’ highlights urgency importance addressing climate change. Declares national energy emergency increase drilling use oil gas. Ends Green New Deal revokes electric vehicle mandate. Emphasizes using country’s oil gas reserves boost economy. example, demonstrated use stance() task stance detection texts regarding “Climate Change”. also showed customize task include additional fields confidence level key arguments supporting stance. Now turn explore stance detection texts topics interest!","code":"custom_stance <- task(   name = \"Custom stance detection\",   system_prompt = paste0(     \"You are an expert annotator. Read each short text carefully and determine its stance towards \",     topic,     \". Classify the stance as Pro, Neutral, or Contra, provide a brief explanation for your classification, and indicate your confidence level from 0 to 1.\"   ),   type_def = ellmer::type_object(     stance = ellmer::type_string(\"Stance towards the topic: Pro, Neutral, or Contra\"),     explanation = ellmer::type_string(\"Brief explanation of the classification\"),     confidence = ellmer::type_number(\"Confidence level from 0 to 1\")   ),   input_type = \"text\" ) # Apply the custom stance task custom_result <- annotate(data_corpus_inaugural, task = custom_stance,                            chat_fn = chat_openai, model = \"gpt-4o\",                           api_args = list(temperature = 0)) ## Running task 'Custom stance detection' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% argument_stance <- task(   name = \"Argument-based stance detection\",   system_prompt = paste0(     \"You are an expert annotator. Read each short text carefully and determine its stance towards \",     topic,     \". Classify the stance as Pro, Neutral, or Contra, provide a brief explanation for your classification, and list up to three key arguments supporting the stance.\"   ),   type_def = ellmer::type_object(     stance = ellmer::type_string(\"Stance towards the topic: Pro, Neutral, or Contra\"),     explanation = ellmer::type_string(\"Brief explanation of the classification\"),     arguments = ellmer::type_string(\"Key arguments supporting the stance\")   ),   input_type = \"text\" ) # Apply the argument-based stance task argument_result <- annotate(data_corpus_inaugural, task = argument_stance,                              chat_fn = chat_openai, model = \"gpt-4o\",                             api_args = list(temperature = 0)) ## Running task 'Argument-based stance detection' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"launching-the-agreement-app","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Launching the Agreement App","title":"Using the Agreement App","text":"launch Agreement App, can use agreement_app() function quallmer package. Make sure package loaded R environment. , simply call agreement_app() function. open Agreement App new window tab web browser.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"using-the-agreement-app-for-manual-coding","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Using the Agreement App for manual coding","title":"Using the Agreement App","text":"Agreement App launched, can start uploading dataset. app supports .csv .rds file formats. uploading data, can select column containing content (e.g., texts, images, etc.) want manually assess. using app, can manually assign score comments text item based coding scheme. can also save example sentences text item help remember coding decisions later illustrative examples research.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"reviewing-llm-generated-annotations","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Reviewing LLM-generated annotations","title":"Using the Agreement App","text":"previously used quallmer package generate annotations using large language models (LLMs), can upload annotations Agreement App review. app allows check LLM-generated codes alongside justifications provided model. can decide whether accept annotations valid invalid, modify based assessment adding comments example sentences.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"saving-your-coding-decisions","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Saving your coding decisions","title":"Using the Agreement App","text":"app provides intuitive interface navigating data making coding decisions. coding decisions saved automatically, find newly created folder named “agreement” working directory.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"calculating-agreement-scores","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Calculating agreement scores","title":"Using the Agreement App","text":"completing manual coding reviewing LLM-generated annotations, Agreement App provides functionality calculate intercoder reliability scores. can choose various metrics, Krippendorff’s alpha, Cohen’s Fleiss’ kappa, assess agreement different coders manual codes LLM annotations , shown , multiple LLM runs. app also provides interpretation guidelines help understand results.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/agreement.html","id":"calculating-agreement-scores-without-the-app","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Calculating agreement scores without the App","title":"Using the Agreement App","text":"addition using Agreement App, can also calculate agreement scores programmatically using agreement() function quallmer package. function allows specify dataset, column containing unit IDs, columns containing coder annotations. ’s example use agreement() function: return calculated agreement scores based specified coders LLM runs.","code":"results <- agreement(   data        = your_data,   unit_id_col = \"doc_id\",   coder_cols  = c(\"coder1\", \"coder2\", \"llm_run1\", \"llm_run2\") ) results"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/customtask.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Loading packages and data","title":"Defining custom tasks","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60]"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/customtask.html","id":"defining-a-custom-prompt","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Defining a custom prompt","title":"Defining custom tasks","text":"Defining prompts crucial step creating custom tasks. prompt guides LLM interpret input data kind output generate. example, create prompt instructs LLM score documents based alignment political left ideologies. Prompts can much longer complex depending task hand. Prompts clear specific ensure LLM understands task requirements.","code":"prompt <- \"Score the following document on a scale of how much it aligns with the political left. The political left is defined as groups which advocate for social equality, government intervention in the economy, and progressive policies. Use the following metrics: SCORING METRIC: 3 : extremely left 2 : very left 1 : slightly left 0 : not at all left\""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/customtask.html","id":"defining-the-structure-of-the-response-with-define_task","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Defining the structure of the response with define_task()","title":"Defining custom tasks","text":"task() function allows us specify expected structure LLM’s response. following important arguments users need specify: name: descriptive name task. system_prompt: prompt guides LLM perform task. type_def: Defines expected structure response using ellmers type specifications type_object(), type_array(), etc. information use ellmer’s type specifications, please refer ellmer documentation type specifications.","code":"# Define the custom task using task() ideology_scores <- task(   name = \"Score Political Left Alignment\",   system_prompt = prompt,   type_def = type_object(     score = type_number(\"Score\"),     explanation = type_string(\"Explanation\")   ),   input_type = \"text\" )"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/customtask.html","id":"applying-the-custom-task-to-the-corpus","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Applying the custom task to the corpus","title":"Defining custom tasks","text":"step similar applying predefined tasks using annotate() function. , use annotate() function apply custom task sample corpus inaugural speeches. specify LLM use (case, openai’s gpt-4o model) additional API arguments needed. example, set temperature 0 deterministic outputs, improving consistency scoring across multiple runs therefore increasing reliability. Now successfully created applied custom annotation task using quallmer package! can modify prompt response structure suit specific research needs.","code":"# Apply the custom task to the inaugural speeches corpus result <- annotate(data_corpus_inaugural, task = ideology_scores,                    chat_fn = chat_openai, model = \"gpt-4o\",                    api_args = list(temperature = 0)) ## Running task 'Score Political Left Alignment' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/openai.html","id":"precautions","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Precautions","title":"Signing up for an openai API key","text":"Closed LLMs OpenAI’s GPT-4o free use require subscription. also come ethical concerns risks, especially comes data privacy security. Therefore, always aware data use potential consequences analysis make sure enable necessary safeguards protect privacy security. addition, aware license use OpenAI models comes along adhering specific regulations avoid misuse. Therefore, always aware data use potential consequences analysis make sure enable necessary safeguards protect privacy security.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/openai.html","id":"setting-up-an-api-key-for-openai-models","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Setting up an API key for openai models","title":"Signing up for an openai API key","text":"openai API provides access various called closed models (fee-based, open-source). services available (example, Claude Google Gemini, etc.) require slightly different set-ups. can find information pricing openai .","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/openai.html","id":"to-use-the-openai-api-please-follow-these-steps","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Setting up an API key for openai models","what":"To use the openai API, please follow these steps:","title":"Signing up for an openai API key","text":"Go openai playground: https://platform.openai.com/playground> Click Sign top right corner Fill details confirm Sign Click now Settings icon top right corner Go Billing provide billing information (otherwise won’t work!) billing information complete, can create new project (top left corner, click Default Project) Click Dashboard top right corner Click API keys left side panel bottom Click Create new secret key Copy key close window copied key, save somewhere safe accessible. security reasons, won’t able view openai account. lose , need regenerate . Keep API key safe share others. suspect key compromised, can regenerate dashboard. aware charged usage API via key.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/openai.html","id":"configuring-your-openai-api-key-in-rstudio","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Setting up an API key for openai models","what":"Configuring your openai API key in RStudio","title":"Signing up for an openai API key","text":"interact openai API, ’s required valid OPENAI_API_KEY environment variable R. can establish environment variable globally including -called .Renviron file. approach ensures environment variable persists across R sessions Shiny app runs background. set commands open .Renviron file modification: Add following line .Renviron, replacing “APIKEY” actual API key: OPENAI_API_KEY=“APIKEY”. need restart R session changes take effect. can clicking Session menu RStudio selecting Restart R. Caution: ’re using version control systems like GitHub GitLab, remember include .Renviron .gitignore file prevent exposing API key! maintain privacy data using gptstudio, highlight, include prompt, otherwise upload sensitive data, code, text remain confidential. Now ready use openai models quallmer package! example, can test setup running example sentiment analysis.","code":"require(usethis) edit_r_environ()"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"loading-packages-and-data-and-defining-a-task","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Loading packages and data and defining a task","title":"The quallmer trail","text":"start loading necessary packages sample dataset. define custom task want LLMs perform.","code":""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"loading-packages-and-data","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Loading packages and data and defining a task","what":"Loading packages and data","title":"The quallmer trail","text":"","code":"# We will use the quanteda package  # for loading a sample corpus of innaugural speeches # If you have not yet installed the quanteda package, you can do so by: # install.packages(\"quanteda\") library(quanteda) ## Package version: 4.3.1 ## Unicode version: 15.1 ## ICU version: 74.2 ## Parallel computing: disabled ## See https://quanteda.io for tutorials and examples. library(quallmer) ## Loading required package: ellmer # For educational purposes,  # we will use a subset of the inaugural speeches corpus # The three most recent speeches in the corpus data_corpus_inaugural <- quanteda::data_corpus_inaugural[57:60] # turn corpus into data frame data_corpus_inaugural <- quanteda::convert(data_corpus_inaugural, to = \"data.frame\")"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"defining-a-custom-prompt","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Loading packages and data and defining a task","what":"Defining a custom prompt","title":"The quallmer trail","text":"similar defining custom prompt annotate() function. , define prompt instructs LLM score documents based alignment political left.","code":"prompt <- \"Score the following document on a scale of how much it aligns with the political left. The political left is defined as groups which advocate for social equality, government intervention in the economy, and progressive policies. Use the following metrics: SCORING METRIC: 3 : extremely left 2 : very left 1 : slightly left 0 : not at all left\""},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"defining-the-structure-of-the-response-with-define_task","dir":"Articles > Pkgdown > Tutorials","previous_headings":"Loading packages and data and defining a task","what":"Defining the structure of the response with define_task()","title":"The quallmer trail","text":"task() function allows us specify expected structure LLM’s response. following important arguments users need specify: name: descriptive name task. system_prompt: prompt guides LLM perform task. type_def: Defines expected structure response using ellmers type specifications type_object(), type_array(), etc. information use ellmer’s type specifications, please refer ellmer documentation type specifications.","code":"# Define the custom task using task() ideology_scores <- task(   name = \"Score Political Left Alignment\",   system_prompt = prompt,   type_def = type_object(     score = type_number(\"Score\"),     explanation = type_string(\"Explanation\")   ),   input_type = \"text\" )"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"define-different-trail-settings","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Define different trail settings","title":"The quallmer trail","text":"defined task, can now set different trail settings compare different LLM configurations affect results. , can use trail_setting() function. example, create four settings different models different temperature values see affect LLM’s responses.","code":"setting_gptmini0 <- trail_setting( provider = \"openai\", model = \"gpt-4.1-mini\", temperature = 0 )  setting_gptmini7 <- trail_setting( provider = \"openai\", model = \"gpt-4.1-mini\", temperature = 0.7 )  setting_gpt400 <- trail_setting( provider = \"openai\", model = \"gpt-4o\", temperature = 0 )  setting_gpt407 <- trail_setting( provider = \"openai\", model = \"gpt-4o\", temperature = 0.7 )"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"run-a-single-trail-with-a-specific-setting","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Run a single trail with a specific setting","title":"The quallmer trail","text":"can use trail_record() function run single trail specific setting. useful ensuring reproducibility results given configuration. example shows run trail using setting_T0 defined earlier. result function data frame containing LLM-generated annotations document dataset well associated metadata setting used, task, data, timestamp, etc. can save output future reference ensure can reproduce results later. can also share output others allow verify findings.","code":"rec_T0 <- trail_record( data = data_corpus_inaugural, text_col = \"text\", task = ideology_scores, setting = setting_gptmini0, id_col = \"doc_id\" ) ## Running task 'Score Political Left Alignment' using model: gpt-4.1-mini ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% # Display the recorded trail's settings library(dplyr) ##  ## Attaching package: 'dplyr' ## The following objects are masked from 'package:stats': ##  ##     filter, lag ## The following objects are masked from 'package:base': ##  ##     intersect, setdiff, setequal, union library(kableExtra) ##  ## Attaching package: 'kableExtra' ## The following object is masked from 'package:dplyr': ##  ##     group_rows meta_df <- tibble(   field = names(rec_T0$meta),   value = vapply(rec_T0$meta, function(x) {     if (is.list(x)) {       # pretty-print lists       paste(capture.output(str(x, max.level = 1)), collapse = \"<br>\")     } else if (length(x) > 1) {       paste(x, collapse = \", \")     } else {       as.character(x)     }   }, FUN.VALUE = character(1)) )  meta_df %>%   kable(\"html\", escape = FALSE, col.names = c(\"Field\", \"Value\")) %>%   kable_styling(bootstrap_options = c(\"striped\", \"hover\"), full_width = FALSE) %>%   column_spec(1, bold = TRUE)"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"run-multiple-trails-with-different-settings","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Run multiple trails with different settings","title":"The quallmer trail","text":"step involves running task data across multiple settings using trail_compare() function. allows us see different configurations impact LLM’s outputs.","code":"left_trails <- trail_compare( data = data_corpus_inaugural, text_col = \"text\", task = ideology_scores, settings = list(   T0 = setting_gptmini0,   T07 = setting_gptmini7,   T40 = setting_gpt400,   T407 = setting_gpt407 ), id_col = \"doc_id\" ) ## Running task 'Score Political Left Alignment' using model: gpt-4.1-mini ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% ## Running task 'Score Political Left Alignment' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100% ## Running task 'Score Political Left Alignment' using model: gpt-4o ## [working] (0 + 0) -> 3 -> 1 | ■■■■■■■■■                         25% ## [working] (0 + 0) -> 1 -> 3 | ■■■■■■■■■■■■■■■■■■■■■■■           75% ## [working] (0 + 0) -> 0 -> 4 | ■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■■  100%"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"build-the-output-by-trail-matrix","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Build the output-by-trail matrix","title":"The quallmer trail","text":"step combines outputs different trails matrix format, row represents document column represents output specific trail.","code":"trail_mat <- trail_matrix( x = left_trails, id_col = \"doc_id\", label_col = \"score\" )  # Display the trail matrix  trail_mat ## # A tibble: 4 × 5 ##   doc_id        T0   T07   T40  T407 ##   <chr>      <dbl> <dbl> <dbl> <dbl> ## 1 2013-Obama     2     2     2     2 ## 2 2017-Trump     0     0     0     0 ## 3 2021-Biden     1     1     2     2 ## 4 2025-Trump     0     0     0     0"},{"path":"https://seraphinem.github.io/quallmer/articles/pkgdown/tutorials/trail.html","id":"compute-agreement-across-trails","dir":"Articles > Pkgdown > Tutorials","previous_headings":"","what":"Compute agreement across trails","title":"The quallmer trail","text":"step assesses stability reliability LLM-generated annotations across different trails using intercoder reliability metrics. output shows intercoder reliability metrics across different trails, indicating consistent LLM-generated annotations across various settings. Higher values suggest greater reliability stability annotations. Overall, trail functionality quallmer package provides systematic way assess reproducibility reliability LLM-generated annotations comparing results across multiple runs different configurations. particularly useful researchers want ensure findings robust overly dependent specific LLM settings. also helps decision model settings use given annotation task.","code":"trail_icr <- trail_agreement( x = left_trails, id_col = \"doc_id\", label_col = \"score\" )  trail_icr ##                            metric  value ## 1                  units_included 4.0000 ## 2                          coders 4.0000 ## 3                      categories 3.0000 ## 4         percent_unanimous_units 0.7500 ## 5 mean_pairwise_percent_agreement 0.8333 ## 6      mean_pairwise_cohens_kappa 0.7333 ## 7             kripp_alpha_nominal 0.7251 ## 8                    fleiss_kappa 0.7193"},{"path":"https://seraphinem.github.io/quallmer/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Seraphine F. Maerz. Author, maintainer. Kenneth Benoit. Author.","code":""},{"path":"https://seraphinem.github.io/quallmer/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Maerz S, Benoit K (2025). quallmer: Qualitative analysis large language models. R package version 0.1.0, https://SeraphineM.github.io/quallmer.","code":"@Manual{,   title = {quallmer: Qualitative analysis with large language models},   author = {Seraphine F. Maerz and Kenneth Benoit},   year = {2025},   note = {R package version 0.1.0},   url = {https://SeraphineM.github.io/quallmer}, }"},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"quallmer-","dir":"","previous_headings":"","what":"Qualitative analysis with large language models","title":"Qualitative analysis with large language models","text":"quallmer package easy--use toolbox qualitative researchers quickly apply AI-assisted annotation texts, images, pdfs, tabular data structured data. Using annotate(), users can generate structured, interpretable outputs powered large language models (LLMs). package includes library predefined tasks common qualitative coding needs, sentiment analysis, thematic coding, stance detection. also allows users create custom annotation tasks tailored specific research questions data types using task(). ensure quality reliability AI-generated annotations, quallmer offers tools comparing LLM outputs human-coded data assessing inter-coder reliability. agreement(), users can launch interactive app manually code data, review AI annotations, evaluate intercoder reliability coders agreement LLM-generated scores. quallmer package makes AI-assisted qualitative coding accessible without requiring deep expertise R, programming machine learning.","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"core-functions","dir":"","previous_headings":"","what":"Core functions","title":"Qualitative analysis with large language models","text":"package provides following core functions:","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"annotate","dir":"","previous_headings":"","what":"annotate()","title":"Qualitative analysis with large language models","text":"generic function works LLM supported ellmer. Generates structured responses based predefined user-defined tasks.","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"task","dir":"","previous_headings":"","what":"task()","title":"Qualitative analysis with large language models","text":"Creates custom annotation tasks tailored specific research questions data types. Uses system_prompt various type specifications ellmer package define LLM interpret inputs format outputs. Tasks created task() can passed directly annotate(). allows users tailor annotation process specific data types makes package extensible future use cases.","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"agreement_app","dir":"","previous_headings":"","what":"agreement_app()","title":"Qualitative analysis with large language models","text":"Manually code data Review validate LLM-generated annotations Compare human-coded data LLM-generated annotations evaluate agreement inter-coder reliability various metrics (e.g., Krippendorff’s alpha, Cohen’s Fleiss’ kappa).","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"agreement","dir":"","previous_headings":"","what":"agreement()","title":"Qualitative analysis with large language models","text":"Calculates intercoder reliability scores (e.g., Krippendorff’s alpha Cohen’s Fleiss’ kappa) multiple human coders human coders LLM-generated annotations. Works similar Agreement App can used programmatically without launching app.","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"extra-the-quallmer-trail-","dir":"","previous_headings":"","what":"EXTRA: The quallmer trail","title":"Qualitative analysis with large language models","text":"Apart core functions , quallmer package also provides set functions ensure reproducibility reliability LLM-generated annotations systematic comparisons across multiple LLM runs different settings. “trail” functionality adds reproducibility layer top annotate() following workflow: Define different trail settings Describe trail, .e., LLMs called (e.g., model, temperature). trail_setting() ↓ Record single LLM trail Record single LLM runs given task specific setting (good reproducibility). trail_record(data, text_col, task, setting) ↓ Run multiple trails different settings Run task data across multiple settings (e.g., different LLMs, different temperatures). trail_compare(data, text_col, task, settings = list(...)) ↓ Build output––trail matrix Treat trail unique “output” combine outputs. trail_matrix(trail_compare_obj) ↓ Compute agreement across trails Assess stability / reliability across multiple LLM runs task different settings. trail_agreement(trail_compare_obj)","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"supported-llms","dir":"","previous_headings":"","what":"Supported LLMs","title":"Qualitative analysis with large language models","text":"package supports LLMs currently available ellmer package. authentication usage LLMs, please refer respective ellmer documentation see tutorial setting openai API key getting started open-source Ollama model.","code":""},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Qualitative analysis with large language models","text":"can install development version quallmer https://github.com/SeraphineM/quallmer :","code":"# install.packages(\"pak\") pak::pak(\"SeraphineM/quallmer\")"},{"path":"https://seraphinem.github.io/quallmer/index.html","id":"example-use-and-tutorials","dir":"","previous_headings":"","what":"Example use and tutorials","title":"Qualitative analysis with large language models","text":"learn use package, please refer step--step tutorials illustrations use predefined tasks.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute intercoder reliability statistics — agreement","title":"Compute intercoder reliability statistics — agreement","text":"function computes set intercoder reliability statistics nominal coding data multiple coders: Krippendorff's alpha (nominal), Fleiss' kappa, mean pairwise Cohen's kappa, mean pairwise percent agreement, share unanimous units, basic counts.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute intercoder reliability statistics — agreement","text":"","code":"agreement(data, unit_id_col, coder_cols, min_coders = 2L)"},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute intercoder reliability statistics — agreement","text":"data data frame containing unit identifier coder columns. unit_id_col Character scalar. Name column identifying units (e.g. document ID, paragraph ID). coder_cols Character vector. Names columns containing coders' codes (column = one coder). min_coders Integer: minimum number non-missing coders per unit unit included. Default 2.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute intercoder reliability statistics — agreement","text":"data frame two columns: metric Name statistic value Estimated value","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Compute intercoder reliability statistics — agreement","text":"","code":"if (FALSE) { # \\dontrun{ agreement(   data = my_df,   unit_id_col = \"doc_id\",   coder_cols  = c(\"coder1\", \"coder2\", \"coder3\") ) } # }"},{"path":"https://seraphinem.github.io/quallmer/reference/agreement_app.html","id":null,"dir":"Reference","previous_headings":"","what":"Launch the Agreement App — agreement_app","title":"Launch the Agreement App — agreement_app","text":"Starts Shiny app manual coding, LLM checking, agreement calculation. - LLM mode, can also select metadata columns. - Agreement mode, select unit ID coder columns (text column).","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/agreement_app.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Launch the Agreement App — agreement_app","text":"","code":"agreement_app()"},{"path":"https://seraphinem.github.io/quallmer/reference/agreement_app.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Launch the Agreement App — agreement_app","text":"shiny.appobj","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/annotate.html","id":null,"dir":"Reference","previous_headings":"","what":"Apply an annotation task to input data — annotate","title":"Apply an annotation task to input data — annotate","text":"Automatically detects correct task type (e.g., text, image). Delegates actual processing task's internal run() method.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/annotate.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Apply an annotation task to input data — annotate","text":"","code":"annotate(.data, task, ...)"},{"path":"https://seraphinem.github.io/quallmer/reference/annotate.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Apply an annotation task to input data — annotate","text":".data Input data (text, image, etc.) task task created [task()] ... Additional arguments passed task$run()","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/annotate.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Apply an annotation task to input data — annotate","text":"Structured data frame results","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for trail_compare — print.trail_compare","title":"Print method for trail_compare — print.trail_compare","text":"Print method trail_compare","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for trail_compare — print.trail_compare","text":"","code":"# S3 method for class 'trail_compare' print(x, ...)"},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for trail_compare — print.trail_compare","text":"x trail_compare object. ... Ignored.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_setting.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for trail_setting — print.trail_setting","title":"Print method for trail_setting — print.trail_setting","text":"Print method trail_setting","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_setting.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for trail_setting — print.trail_setting","text":"","code":"# S3 method for class 'trail_setting' print(x, ...)"},{"path":"https://seraphinem.github.io/quallmer/reference/print.trail_setting.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for trail_setting — print.trail_setting","text":"x trail_setting object. ... Ignored.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/quallmer-package.html","id":null,"dir":"Reference","previous_headings":"","what":"quallmer: Qualitative analysis with large language models — quallmer-package","title":"quallmer: Qualitative analysis with large language models — quallmer-package","text":"Provides userfriendly support qualitative analysis large language models (LLMs).","code":""},{"path":[]},{"path":"https://seraphinem.github.io/quallmer/reference/quallmer-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"quallmer: Qualitative analysis with large language models — quallmer-package","text":"Maintainer: Seraphine F. Maerz seraphine.maerz@unimelb.edu.au (ORCID) Authors: Kenneth Benoit kbenoit@smu.edu.sg (ORCID)","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task.html","id":null,"dir":"Reference","previous_headings":"","what":"Define an annotation task — task","title":"Define an annotation task — task","text":"flexible task definition wrapper ellmer. Supports structured output type, including `type_object()`, `type_array()`, `type_enum()`, `type_boolean()`, others.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define an annotation task — task","text":"","code":"task(name, system_prompt, type_def, input_type = \"text\")"},{"path":"https://seraphinem.github.io/quallmer/reference/task.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define an annotation task — task","text":"name Name task. system_prompt System prompt guide model (required ellmer's `chat_fn`). type_def Structured output definition, e.g., created `ellmer::type_object()`, `ellmer::type_array()`, `ellmer::type_enum()`. input_type Type input data: `\"text\"`, `\"image\"`, `\"audio\"`, etc.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define an annotation task — task","text":"task object `run()` method.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_fact.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for overall truthfulness assessment — task_fact","title":"Predefined task for overall truthfulness assessment — task_fact","text":"Assigns overall truthfulness score text lists topics reduce confidence accuracy.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_fact.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for overall truthfulness assessment — task_fact","text":"","code":"task_fact(max_topics = 5)"},{"path":"https://seraphinem.github.io/quallmer/reference/task_fact.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predefined task for overall truthfulness assessment — task_fact","text":"max_topics Integer: maximum number topics issues list reducing confidence truthfulness text. Default 5.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_fact.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for overall truthfulness assessment — task_fact","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_ideology.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for ideological scaling on a specified dimension — task_ideology","title":"Predefined task for ideological scaling on a specified dimension — task_ideology","text":"Ideological scaling specified dimension, justification.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_ideology.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for ideological scaling on a specified dimension — task_ideology","text":"","code":"task_ideology(   dimension = \"the specified ideological dimension (0 = first pole, 10 = second pole)\",   definition = NULL )"},{"path":"https://seraphinem.github.io/quallmer/reference/task_ideology.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predefined task for ideological scaling on a specified dimension — task_ideology","text":"dimension character string specifying ideological dimension, ideally naming poles, e.g., \"liberal - illiberal\", \"left - right\", \"inclusive - exclusive\". first pole corresponds 0 second 10. definition Optional detailed explanation dimension means. provided, included system prompt guide annotation.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_ideology.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for ideological scaling on a specified dimension — task_ideology","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_salience.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for salience of topics discussed (ranked topics) — task_salience","title":"Predefined task for salience of topics discussed (ranked topics) — task_salience","text":"Ranked list topics mentioned text, ordered salience.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_salience.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for salience of topics discussed (ranked topics) — task_salience","text":"","code":"task_salience(topics = NULL, max_topics = 5)"},{"path":"https://seraphinem.github.io/quallmer/reference/task_salience.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predefined task for salience of topics discussed (ranked topics) — task_salience","text":"topics Optional character vector predefined topic labels (e.g., c(\"economy\", \"health\", \"education\", \"environment\")). supplied, model classify rank among topics. NULL, model may infer topic labels directly text. max_topics Integer: maximum number topics return topics inferred text. Default 5.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_salience.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for salience of topics discussed (ranked topics) — task_salience","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_sentiment.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for sentiment analysis — task_sentiment","title":"Predefined task for sentiment analysis — task_sentiment","text":"Predefined task sentiment analysis","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_sentiment.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for sentiment analysis — task_sentiment","text":"","code":"task_sentiment()"},{"path":"https://seraphinem.github.io/quallmer/reference/task_sentiment.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for sentiment analysis — task_sentiment","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_stance.html","id":null,"dir":"Reference","previous_headings":"","what":"Predefined task for stance detection (position taking) — task_stance","title":"Predefined task for stance detection (position taking) — task_stance","text":"Predefined task stance detection (position taking)","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_stance.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predefined task for stance detection (position taking) — task_stance","text":"","code":"task_stance(topic = \"the given topic\")"},{"path":"https://seraphinem.github.io/quallmer/reference/task_stance.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predefined task for stance detection (position taking) — task_stance","text":"topic character string specifying topic stance detection.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/task_stance.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predefined task for stance detection (position taking) — task_stance","text":"task object","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_agreement.html","id":null,"dir":"Reference","previous_headings":"","what":"Compute agreement across Trail settings — trail_agreement","title":"Compute agreement across Trail settings — trail_agreement","text":"Convenience helper compute intercoder reliability across multiple Trail records Trail comparison treating setting coder.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_agreement.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Compute agreement across Trail settings — trail_agreement","text":"","code":"trail_agreement(   x,   id_col = \"id\",   label_col = \"label\",   min_coders = 2L,   agreement_fun = agreement )"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_agreement.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Compute agreement across Trail settings — trail_agreement","text":"x trail_compare object list trail_record objects. id_col Character scalar. Name unit identifier column resulting wide data (defaults \"id\"). label_col Character scalar. Name label column record's annotations (defaults \"label\"). min_coders Integer. Minimum number non-missing coders per unit required inclusion. Passed agreement_fun. agreement_fun Function used compute agreement. Defaults agreement(), expected accept data, unit_id_col, coder_cols, min_coders.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_agreement.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Compute agreement across Trail settings — trail_agreement","text":"result calling agreement_fun() wide data,   typically data frame agreement statistics.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_compare.html","id":null,"dir":"Reference","previous_headings":"","what":"Trail compare: run the same task across multiple settings — trail_compare","title":"Trail compare: run the same task across multiple settings — trail_compare","text":"Apply quallmer task data text column set settings, returning one trail_record per setting.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_compare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trail compare: run the same task across multiple settings — trail_compare","text":"","code":"trail_compare(   data,   text_col,   task,   settings,   id_col = NULL,   cache_dir = \"trail_cache\",   overwrite = FALSE,   annotate_fun = annotate )"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_compare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trail compare: run the same task across multiple settings — trail_compare","text":"data data frame containing text annotated. text_col Character scalar. Name text column. task quallmer task object. settings named list trail_setting objects. names used identifiers setting (e.g. coder IDs). id_col Optional character scalar. Name unit identifier column. NULL, temporary \".trail_unit_id\" created shared across records. cache_dir Optional directory caching. Passed trail_record(). overwrite Logical. TRUE, ignore cache settings recompute. annotate_fun Function used perform annotation, passed trail_record().","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_compare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trail compare: run the same task across multiple settings — trail_compare","text":"object class \"trail_compare\" containing named   list trail_record objects basic metadata.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_matrix.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert Trail records to coder-style wide data — trail_matrix","title":"Convert Trail records to coder-style wide data — trail_matrix","text":"Treat setting/record Trail comparison separate coder convert annotations wide data frame suitable agreement analysis.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_matrix.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert Trail records to coder-style wide data — trail_matrix","text":"","code":"trail_matrix(x, id_col = \"id\", label_col = \"label\")"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_matrix.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert Trail records to coder-style wide data — trail_matrix","text":"x Either trail_compare object named list trail_record objects. id_col Character scalar. Name column identifies units (documents, paragraphs, etc.). Must present record's annotations data. label_col Character scalar. Name column record's annotations data containing code label interest.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_matrix.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert Trail records to coder-style wide data — trail_matrix","text":"data frame one row per unit one column per   setting/record. unit ID column retained name   id_col.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_record.html","id":null,"dir":"Reference","previous_headings":"","what":"Trail record: reproducible quallmer annotation — trail_record","title":"Trail record: reproducible quallmer annotation — trail_record","text":"Run quallmer task data frame specified LLM setting, capturing metadata reproducibility optionally caching full result disk.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_record.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trail record: reproducible quallmer annotation — trail_record","text":"","code":"trail_record(   data,   text_col,   task,   setting,   id_col = NULL,   cache_dir = \"trail_cache\",   overwrite = FALSE,   annotate_fun = annotate )"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_record.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trail record: reproducible quallmer annotation — trail_record","text":"data data frame containing text annotated. text_col Character scalar. Name text column. task quallmer task object. setting trail_setting object describing LLM configuration. id_col Optional character scalar identifying units. cache_dir Optional directory cache Trails. NULL, caching disabled. overwrite Whether overwrite existing cache. annotate_fun Function used perform annotation (default annotate()).","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_record.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trail record: reproducible quallmer annotation — trail_record","text":"object class \"trail_record\".","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_setting.html","id":null,"dir":"Reference","previous_headings":"","what":"Trail setting specification — trail_setting","title":"Trail setting specification — trail_setting","text":"Define reproducible specification LLM setting use quallmer Trail. object captures provider, model name, temperature, optional extra arguments.","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_setting.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Trail setting specification — trail_setting","text":"","code":"trail_setting(   provider = \"openai\",   model = \"gpt-4.1-mini\",   temperature = 0,   extra = list() )"},{"path":"https://seraphinem.github.io/quallmer/reference/trail_setting.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Trail setting specification — trail_setting","text":"provider Character. Backend provider, e.g. \"openai\", \"ollama\", \"azure\". model Character. Model identifier, e.g. \"gpt-4.1-mini\", \"gpt-4o-mini\", \"llama3.1:8b\". temperature Numeric scalar. Sampling temperature (default 0). extra Named list additional model arguments passed `annotate()` via `api_args` needed (e.g. penalties safety flags).","code":""},{"path":"https://seraphinem.github.io/quallmer/reference/trail_setting.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Trail setting specification — trail_setting","text":"object class \"trail_setting\".","code":""}]
