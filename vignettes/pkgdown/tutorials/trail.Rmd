---
title: "The quallmer trail"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In this tutorial, we will explore the quallmer trail system, which automatically captures provenance metadata for full workflow traceability. The trail system helps you:

- Track the complete history of your coding workflow
- Document model parameters and settings used
- Assess robustness of downstream analyses
- Maintain parent-child relationships across coding runs
- Export provenance information for reproducibility
- Generate human-readable reports of your analysis pipeline

## Understanding provenance tracking

All `qlm_coded`, `qlm_comparison`, and `qlm_validation` objects in quallmer automatically capture provenance metadata, including:

- **Run name**: A unique identifier for each coding run
- **Timestamp**: When the coding was executed
- **Model**: The LLM model and parameters used
- **Codebook**: The coding instructions applied
- **Parent runs**: Links to previous runs in a replication chain
- **Metadata**: Package versions, number of units coded, etc.

This metadata enables full workflow traceability.

## Loading packages and data

```{r getting-started}
# We will use the quanteda package
# for loading a sample corpus of inaugural speeches
library(quanteda)
library(quallmer)

# For educational purposes,
# we will use a subset of the inaugural speeches corpus
data_corpus_inaugural <- quanteda::data_corpus_inaugural[50:60]
```

## Creating a workflow with provenance tracking

Let's build a coding workflow that demonstrates the trail system. We'll use sentiment analysis as our example.

### Initial coding run

```{r initial-run}
# For this tutorial, we'll use the built-in sentiment codebook
# For real research, you would create a custom codebook tailored to your
# specific research question (see the "Creating codebooks" tutorial)

# Code with GPT-4o (note the 'name' parameter for tracking)
coded1 <- qlm_code(data_corpus_inaugural,
                   codebook = data_codebook_ideology,
                   model = "openai/gpt-4o",
                   params = params(temperature = 0),
                   name = "initial_gpt4o")

# Each qlm_coded object has a 'run' attribute with metadata
attr(coded1, "run")$name
attr(coded1, "run")$metadata$timestamp
```

### Creating a replication chain

When you use `qlm_replicate()`, the new run automatically links to its parent:

```{r replication-chain}
# Replicate with GPT-4o-mini
coded2 <- qlm_replicate(coded1,
                        model = "openai/gpt-4o-mini",
                        name = "replicate_mini")

# Check the parent relationship
attr(coded2, "run")$parent  # Shows "initial_gpt4o"

# Replicate again with different temperature
coded3 <- qlm_replicate(coded2,
                        params = params(temperature = 0.7),
                        name = "mini_temp07")

# This creates a chain: initial_gpt4o -> replicate_mini -> mini_temp07
attr(coded3, "run")$parent  # Shows "replicate_mini"
```

## Extracting and displaying provenance trails

The `qlm_trail()` function extracts and displays the complete provenance chain from your quallmer objects.

### Viewing a single run

```{r trail-single}
# Extract trail from a single object
trail1 <- qlm_trail(coded1)

# Print the trail
trail1
```

This displays:
- Run name
- Parent (if any)
- Creation timestamp
- Model used

### Reconstructing a complete chain

To see the full provenance chain, provide all objects in the lineage:

```{r trail-chain}
# Provide all objects to reconstruct the complete chain
full_trail <- qlm_trail(coded3, coded2, coded1)

# Print shows the complete history
full_trail
```

The output shows:

- All runs in chronological order
- Parent-child relationships
- Model and parameter changes across runs
- Timestamps for each step
- Codebook used


## Assessing robustness of downstream analysis

The `qlm_trail_robustness()` function helps you assess whether your **substantive findings** change across different models or settings. Instead of just comparing raw coded values (as qlm_compare() already does), it compares the results of your downstream analysis (means, proportions, correlations, etc.).

### Defining your downstream analysis

First, define a function that performs your analysis on coded data:

```{r robustness-define}
# Define what analysis you want to compare
my_analysis <- function(coded) {
  list(
    mean_score = mean(coded$score, na.rm = TRUE),
    sd_score = sd(coded$score, na.rm = TRUE),
    n_units = sum(!is.na(coded$score))
  )
}
```

The analysis function should:
- Take a `qlm_coded` object as input
- Return a named list of numeric statistics
- Each statistic should be a single number (e.g., mean, proportion, correlation)

### Computing robustness

```{r robustness-basic}
# Compute how much your analysis results differ across models
robustness <- qlm_trail_robustness(coded1, coded2, coded3,
                                   reference = "initial_gpt4o",
                                   analysis_fn = my_analysis)

# View the robustness scale
robustness
```

The output shows:

- **run**: Name of each run 
- **statistic**: Which analysis statistic (e.g., mean_score) 
- **value**: Value from this run 
- **reference_value**: Value from the reference run 
- **abs_diff**: Absolute difference from reference 
- **pct_diff**: Percent change from reference 

### Interpreting robustness

```{r robustness-interpret}
# Check which statistics have large differences (>5% change)
concerning <- robustness[abs(robustness$pct_diff) > 5 & !is.na(robustness$pct_diff), ]

# Check which are highly stable (<1% change)
stable <- robustness[abs(robustness$pct_diff) < 1 & !is.na(robustness$pct_diff), ]
```

**Interpretation guidelines:**

- **<1% difference**: Highly robust, findings are very stable 
- **1-5% difference**: Good robustness, minor variations 
- **5-10% difference**: Moderate robustness, some sensitivity 
- **>10% difference**: Low robustness, conclusions may change 

The acceptable threshold depends on your research context and the magnitude of effects you're studying.


## Integrating comparisons and validations

The trail system automatically tracks comparisons and validations as part of your workflow.

### Complete workflow example

```{r workflow-complete}
# 1. Code with two different models
coded_gpt4o <- qlm_code(data_corpus_inaugural,
                        codebook = data_codebook_ideology,
                        model = "openai/gpt-4o",
                        params = params(temperature = 0),
                        name = "gpt4o_run")

coded_mini <- qlm_replicate(coded_gpt4o,
                            model = "openai/gpt-4o-mini",
                            name = "mini_run")

# 2. Compare inter-rater reliability
comparison <- qlm_compare(coded_gpt4o, coded_mini,
                          by = "score",
                          level = "nominal")

# View the comparison results
print(comparison)

# 3. Validate against gold standard (if you have one)
# gold <- data.frame(.id = coded_gpt4o$.id, score = c(2, 3, 1, ...))
# validation <- qlm_validate(coded_gpt4o, gold = gold, by = "score")
# print(validation)
```

### Viewing complete workflow trail

```{r workflow-trail}
# Get full provenance trail including comparisons
full_trail <- qlm_trail(coded_gpt4o, coded_mini, comparison)
print(full_trail)

# The trail shows parent-child relationships:
# - gpt4o_run (original)
# - mini_run (parent: gpt4o_run)
# - comparison_... (parents: gpt4o_run, mini_run)
```

## Exporting and documenting your workflow

You can export your trail for documentation and reproducibility:

```{r trail-export}
# Extract the full trail
trail <- qlm_trail(coded_gpt4o, coded_mini, comparison)

# Save as RDS for archival
# qlm_trail_save(trail, "my_workflow_trail.rds")

# Export as JSON for portability
# qlm_trail_export(trail, "my_workflow_trail.json")

# Generate a human-readable report
# qlm_trail_report(trail, "my_workflow_report.qmd")

# Include comparison metrics in the report
# qlm_trail_report(trail, "my_workflow_report.qmd", include_comparisons = TRUE)

# Include both comparisons and validations
# qlm_trail_report(trail, "my_workflow_report.qmd",
#                  include_comparisons = TRUE,
#                  include_validations = TRUE)
```

The generated report includes:

- Timeline of all coding runs
- Model settings and parameters
- Comparison and validation metrics (if `include_comparisons` or `include_validations` is `TRUE`)
- System information for reproducibility

## Summary

The quallmer trail system automatically tracks your coding workflow:

- **Provenance tracking**: All runs are timestamped with model and parameter information
- **Parent-child links**: `qlm_replicate()` maintains relationships between runs
- **Quality assessment**: Use `qlm_compare()` to check agreement and `qlm_trail_robustness()` to test if conclusions change
- **Documentation**: Export trails for methods sections and replication packages

**Best practices:**

1. Name your runs with the `name` parameter
2. Use `qlm_compare()` to assess inter-rater reliability
3. Use `qlm_trail_robustness()` to check if your findings are stable across models
4. Export trails for transparency and reproducibility
