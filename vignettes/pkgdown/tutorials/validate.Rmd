---
title: "Using the Validate App"
output: html_document
---

The Validate App is a user-friendly interface that allows researchers to manually code data, review LLM-generated annotations, and calculate inter-rater reliability scores such as Krippendorff's alpha and Fleiss' kappa or compare LLM-annotations against a gold standard for accuracy scores. This tutorial will guide you through the steps of using the Validate App effectively. If you prefer to work programmatically, we also provide instructions on how to calculate agreement scores by simply using the `qlm_validate()` or `qlm_compare()` functions at the end of this tutorial.

## Installation

The Validate App is available in the companion package `quallmer.app`. To install it:

```r
# install.packages("pak")
pak::pak("SeraphineM/quallmer.app")
```

## Launching the Validate App
To launch the Validate App, load the `quallmer.app` package and call the `validate_app()` function. This will open the Validate App in a new window or tab in your web browser.

```r
library(quallmer.app)
validate_app()
```

## Using the Validate App for manual coding

Once the Validate App is launched, you can start by uploading your dataset. The app supports .csv or .rds file formats. After uploading your data, you can select the column containing the content (e.g., texts, images, etc.) you want to manually assess. While using the app, you can manually assign a score or comments to each text item based on your coding scheme. You can also save example sentences from each text item to help you remember your coding decisions later or as illustrative examples for your research. 

![](pics/manual.png)

## Reviewing LLM-generated annotations

If you have previously used the `quallmer` package to generate annotations using large language models (LLMs), you can upload those annotations into the Validate App for review. The app allows you to check the LLM-generated codes alongside justifications provided by the model. You can then decide whether to accept these annotations as `valid` or `invalid`, or modify them based on your assessment by adding comments and example sentences.

![](pics/llm.png)


## Saving your coding decisions

The app provides an intuitive interface for navigating through the data and making coding decisions. **All your coding decisions will be saved automatically, you find it in a newly created folder named "agreement" in your working directory.**


## Calculating validation scores

After completing the manual coding and reviewing the LLM-generated annotations, the Validate App provides functionality to calculate inter-rater reliability scores or compare LLM annotations against a gold standard. You can choose from various metrics, such as Krippendorff's alpha, Cohen's or Fleiss' kappa, to assess the agreement between different coders or between your manual codes and the LLM annotations or, as shown below, between multiple LLM runs. The app also provides some interpretation guidelines to help you understand the results. If you have a gold standard available, the app will calculate accuracy metrics such as precision, recall, and F1-score.

### Example of inter-rater reliability scores (no gold standard available)

![](pics/reliab.png){width=70%}

### Example of accuracy assessment against a gold standard

![](pics/accuracy.png){width=70%}


## Calculating validation scores without the App

In addition to using the Validate App, you can also calculate agreement and validation scores programmatically using functions from the `quallmer` package.

### Comparing multiple coders (inter-coder reliability)

If you **do not have a gold standard available** and want to assess inter-coder reliability between multiple `qlm_coded` objects (from different coders, models, or coding runs), use `qlm_compare()`:

```.r
# Compare two or more qlm_coded objects
comparison <- qlm_compare(coded_run1, coded_run2, coded_run3)
comparison
```

This will compute agreement metrics including Krippendorff's alpha, Cohen's kappa, and Fleiss' kappa.

### Validating against a gold standard

If you **have a gold standard available** and want to assess accuracy of LLM-generated annotations against the gold standard, use `qlm_validate()`:

```.r
# Validate LLM coding against gold standard
validation <- qlm_validate(
  coded_object = llm_coded_result,
  gold_standard = gold_standard_data,
  gold_column = "manual_code"
)
validation
```

This will compute classification metrics including accuracy, precision, recall, F1-score, and Cohen's kappa.
