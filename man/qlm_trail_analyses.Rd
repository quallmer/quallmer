% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlm_trail.R
\name{qlm_trail_analyses}
\alias{qlm_trail_analyses}
\title{Extract comparisons and validations from trail}
\usage{
qlm_trail_analyses(...)
}
\arguments{
\item{...}{One or more quallmer objects (\code{qlm_coded}, \code{qlm_comparison},
\code{qlm_validation}). These are the objects that were part of your analysis
workflow.}
}
\value{
A list with two elements:
\describe{
\item{comparisons}{Data frame of comparison results with columns: run,
parents (comma-separated), measure, value, subjects, raters}
\item{validations}{Data frame of validation results with columns: run,
parents (comma-separated), accuracy, precision, recall, f1, kappa}
}
}
\description{
Retrieves all comparison and validation results from a set of quallmer objects,
showing how different runs were compared or validated.
}
\details{
This function helps you see all the reliability and validity assessments
that were performed on your coded data. It extracts the key metrics from
comparison and validation objects and presents them in a summary format.

Use this to quickly review:
\itemize{
\item Inter-rater reliability metrics across different model combinations
\item Validation performance against gold standards
\item Which runs were compared or validated together
}
}
\examples{
\dontrun{
# Create coded versions
coded1 <- qlm_code(texts, codebook, model = "openai/gpt-4o", name = "run1")
coded2 <- qlm_replicate(coded1, model = "openai/gpt-4o-mini", name = "run2")

# Compare them
comparison <- qlm_compare(coded1, coded2, by = "sentiment")

# Validate against gold standard
validation <- qlm_validate(coded1, gold = my_gold, by = "sentiment")

# Extract all comparisons and validations
results <- qlm_trail_analyses(coded1, coded2, comparison, validation)
print(results$comparisons)
print(results$validations)
}

}
\seealso{
\code{\link[=qlm_trail]{qlm_trail()}}, \code{\link[=qlm_compare]{qlm_compare()}}, \code{\link[=qlm_validate]{qlm_validate()}}
}
