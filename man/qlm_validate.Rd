% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlm_validate.R
\name{qlm_validate}
\alias{qlm_validate}
\title{Validate coded results against gold standard}
\usage{
qlm_validate(
  x,
  gold,
  by,
  measure = c("all", "accuracy", "precision", "recall", "f1", "kappa"),
  average = c("macro", "micro", "weighted", "none"),
  level = c("nominal", "ordinal", "interval")
)
}
\arguments{
\item{x}{A \code{qlm_coded} object containing LLM predictions to validate.}

\item{gold}{A data frame containing gold standard annotations. Must include
a \code{.id} column for joining with \code{x} and the variable specified in \code{by}.
(Can also be a qlm_coded object.)}

\item{by}{Character scalar. Name of the variable to validate. Must be present
in both \code{x} and \code{gold}.}

\item{measure}{Character scalar. Which metrics to compute:
\describe{
\item{\code{"all"}}{Compute all metrics (default)}
\item{\code{"accuracy"}}{Overall accuracy only}
\item{\code{"precision"}}{Precision only}
\item{\code{"recall"}}{Recall only}
\item{\code{"f1"}}{F1-score only}
\item{\code{"kappa"}}{Cohen's kappa only}
}}

\item{average}{Character scalar. Averaging method for multiclass metrics:
\describe{
\item{\code{"macro"}}{Unweighted mean across classes (default)}
\item{\code{"micro"}}{Aggregate contributions globally (sum TP, FP, FN)}
\item{\code{"weighted"}}{Weighted mean by class prevalence}
\item{\code{"none"}}{Return per-class metrics in addition to global metrics}
}}

\item{level}{Character scalar. Measurement level of the variable:
\code{"nominal"}, \code{"ordinal"}, \code{"interval"}, or \code{"ratio"}. Default is \code{"nominal"}.}
}
\value{
A \code{qlm_validation} object containing:
\describe{
\item{\code{accuracy}}{Overall accuracy (if computed)}
\item{\code{precision}}{Precision (if computed)}
\item{\code{recall}}{Recall (if computed)}
\item{\code{f1}}{F1-score (if computed)}
\item{\code{kappa}}{Cohen's kappa (if computed)}
\item{\code{by_class}}{Per-class metrics (only when \code{average = "none"})}
\item{\code{confusion}}{Confusion matrix (yardstick conf_mat object)}
\item{\code{n}}{Number of units compared}
\item{\code{classes}}{Class labels}
\item{\code{average}}{Averaging method used}
\item{\code{level}}{Measurement level}
\item{\code{variable}}{Variable name validated}
\item{\code{call}}{Function call}
}
}
\description{
Validates LLM-coded results from a \code{qlm_coded} object against a gold standard
(typically human annotations) using classification metrics from the yardstick
package. Computes accuracy, precision, recall, F1-score, and Cohen's kappa.
}
\details{
The function performs an inner join between \code{x} and \code{gold} using the \code{.id}
column, so only units present in both datasets are included in validation.
Missing values (NA) in either predictions or gold standard are excluded with
a warning.

For multiclass problems, the \code{average} parameter controls how per-class
metrics are aggregated:
\itemize{
\item \strong{Macro averaging} computes metrics for each class independently and takes
the unweighted mean. This treats all classes equally regardless of size.
\item \strong{Micro averaging} aggregates all true positives, false positives, and
false negatives globally before computing metrics. This weights classes by
their prevalence.
\item \strong{Weighted averaging} computes metrics for each class and takes the mean
weighted by class size.
\item \strong{No averaging} (\code{average = "none"}) returns global macro-averaged metrics
plus per-class breakdown.
}
}
\examples{
\dontrun{
# Basic validation against gold standard

set.seed(24)
reviews <- data_corpus_LMRDsample[sample(length(data_corpus_LMRDsample), size = 20)]

# Code movie reviews
coded <- qlm_code(
  reviews,
  data_codebook_sentiment,
  model = "google_gemini/gemini-2.5-flash"
)

# Create gold standard from corpus metadata
gold <- data.frame(
  .id = coded$.id,
  polarity = quanteda::docvars(reviews, "polarity")
)

# Validate
validation <- qlm_validate(coded, gold, by = "polarity")
print(validation)

# Compute only specific metrics
qlm_validate(coded, gold, by = "polarity", measure = "f1")

# Use micro-averaging
qlm_validate(coded, gold, by = "polarity", average = "micro")

# Get per-class breakdown
validation_detailed <- qlm_validate(coded, gold, by = "polarity", average = "none")
print(validation_detailed)
validation_detailed$by_class$precision
}

}
\seealso{
\code{\link[=qlm_compare]{qlm_compare()}} for inter-rater reliability between coded objects,
\code{\link[=qlm_code]{qlm_code()}} for creating coded objects,
\code{\link[yardstick:accuracy]{yardstick::accuracy()}}, \code{\link[yardstick:precision]{yardstick::precision()}}, \code{\link[yardstick:recall]{yardstick::recall()}},
\code{\link[yardstick:f_meas]{yardstick::f_meas()}}, \code{\link[yardstick:kap]{yardstick::kap()}}, \code{\link[yardstick:conf_mat]{yardstick::conf_mat()}}
}
