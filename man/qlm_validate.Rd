% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlm_validate.R
\name{qlm_validate}
\alias{qlm_validate}
\title{Validate coded results against a gold standard}
\usage{
qlm_validate(
  x,
  gold,
  by,
  level = c("nominal", "ordinal", "interval"),
  average = c("macro", "micro", "weighted", "none")
)
}
\arguments{
\item{x}{A \code{qlm_coded} object containing LLM predictions to validate.}

\item{gold}{A data frame containing gold standard annotations. Must include
a \code{.id} column for joining with \code{x} and the variable specified in \code{by}.
(Can also be a qlm_coded object.)}

\item{by}{Character scalar. Name of the variable to validate. Must be present
in both \code{x} and \code{gold}.}

\item{level}{Character scalar. Measurement level of the variable: \code{"nominal"},
\code{"ordinal"}, or \code{"interval"}. Default is \code{"nominal"}. Determines which
validation metrics are computed.}

\item{average}{Character scalar. Averaging method for multiclass metrics
(nominal level only):
\describe{
\item{\code{"macro"}}{Unweighted mean across classes (default)}
\item{\code{"micro"}}{Aggregate contributions globally (sum TP, FP, FN)}
\item{\code{"weighted"}}{Weighted mean by class prevalence}
\item{\code{"none"}}{Return per-class metrics in addition to global metrics}
}}
}
\value{
A \code{qlm_validation} object containing:
\describe{
\item{\code{accuracy}}{Overall accuracy (nominal only)}
\item{\code{precision}}{Precision (nominal only)}
\item{\code{recall}}{Recall (nominal only)}
\item{\code{f1}}{F1-score (nominal only)}
\item{\code{kappa}}{Cohen's kappa (nominal only)}
\item{\code{rho}}{Spearman's rho rank correlation (ordinal only)}
\item{\code{tau}}{Kendall's tau rank correlation (ordinal only)}
\item{\code{r}}{Pearson's r correlation (interval only)}
\item{\code{icc}}{Intraclass correlation coefficient (interval only)}
\item{\code{mae}}{Mean absolute error (ordinal/interval)}
\item{\code{rmse}}{Root mean squared error (interval only)}
\item{\code{by_class}}{Per-class metrics (nominal with \code{average = "none"} only)}
\item{\code{confusion}}{Confusion matrix (nominal only)}
\item{\code{n}}{Number of units compared}
\item{\code{classes}}{Class/level labels}
\item{\code{average}}{Averaging method used}
\item{\code{level}}{Measurement level}
\item{\code{variable}}{Variable name validated}
\item{\code{call}}{Function call}
}
}
\description{
Validates LLM-coded results from a \code{qlm_coded} object against a gold standard
(typically human annotations) using appropriate metrics based on measurement
level. For nominal data, computes accuracy, precision, recall, F1-score, and
Cohen's kappa. For ordinal data, computes accuracy and weighted kappa (linear
weighting), which accounts for the ordering and distance between categories.
}
\details{
The function performs an inner join between \code{x} and \code{gold} using the \code{.id}
column, so only units present in both datasets are included in validation.
Missing values (NA) in either predictions or gold standard are excluded with
a warning.

\strong{Measurement levels:}
\itemize{
\item \strong{Nominal}: Categories with no inherent ordering (e.g., topics, sentiment
polarity). Metrics: accuracy, precision, recall, F1-score, Cohen's kappa
(unweighted).
\item \strong{Ordinal}: Categories with meaningful ordering but unequal intervals
(e.g., ratings 1-5, Likert scales). Metrics: Spearman's rho (\code{rho}, rank
correlation), Kendall's tau (\code{tau}, rank correlation), and MAE (\code{mae}, mean
absolute error). These measures account for the ordering of categories
without assuming equal intervals.
\item \strong{Interval/Ratio}: Numeric data with equal intervals (e.g., counts,
continuous measurements). Metrics: ICC (intraclass correlation), Pearson's r
(linear correlation), MAE (mean absolute error), and RMSE (root mean squared
error).
}

For multiclass problems with nominal data, the \code{average} parameter controls
how per-class metrics are aggregated:
\itemize{
\item \strong{Macro averaging} computes metrics for each class independently and takes
the unweighted mean. This treats all classes equally regardless of size.
\item \strong{Micro averaging} aggregates all true positives, false positives, and
false negatives globally before computing metrics. This weights classes by
their prevalence.
\item \strong{Weighted averaging} computes metrics for each class and takes the mean
weighted by class size.
\item \strong{No averaging} (\code{average = "none"}) returns global macro-averaged metrics
plus per-class breakdown.
}

Note: The \code{average} parameter only affects precision, recall, and F1 for
nominal data. For ordinal data, these metrics are not computed.
}
\examples{
\dontrun{
# Basic validation against gold standard

set.seed(24)
reviews <- data_corpus_LMRDsample[sample(length(data_corpus_LMRDsample), size = 20)]

# Code movie reviews
coded <- qlm_code(
  reviews,
  data_codebook_sentiment,
  model = "openai/gpt-4o"
)

# Create gold standard from corpus metadata
gold <- data.frame(
  .id = coded$.id,
  sentiment = quanteda::docvars(reviews, "polarity"),
  rating = quanteda::docvars(reviews, "rating")
)

# Validate polarity (nominal data)
validation <- qlm_validate(coded, gold, by = "sentiment", level = "nominal")
print(validation)

# Validate ratings (ordinal data)
validation_ordinal <- qlm_validate(coded, gold_ratings, by = "rating", level = "ordinal")
print(validation_ordinal)

# Use micro-averaging (nominal level only)
qlm_validate(coded, gold, by = "sentiment", level = "nominal", average = "micro")

# Get per-class breakdown (for nominal data only)
validation_detailed <- qlm_validate(coded, gold, by = "sentiment",
                                    level = "nominal", average = "none")
print(validation_detailed)
validation_detailed$by_class
validation_detailed$confusion
}

}
\seealso{
\code{\link[=qlm_compare]{qlm_compare()}} for inter-rater reliability between coded objects,
\code{\link[=qlm_code]{qlm_code()}} for creating coded objects,
\code{\link[yardstick:accuracy]{yardstick::accuracy()}}, \code{\link[yardstick:precision]{yardstick::precision()}}, \code{\link[yardstick:recall]{yardstick::recall()}},
\code{\link[yardstick:f_meas]{yardstick::f_meas()}}, \code{\link[yardstick:kap]{yardstick::kap()}}, \code{\link[yardstick:conf_mat]{yardstick::conf_mat()}}
}
