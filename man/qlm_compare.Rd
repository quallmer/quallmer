% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlm_compare.R
\name{qlm_compare}
\alias{qlm_compare}
\title{Compare coded results for inter-rater reliability}
\usage{
qlm_compare(
  ...,
  by,
  level = c("nominal", "ordinal", "interval", "ratio"),
  tolerance = 0
)
}
\arguments{
\item{...}{Two or more \code{qlm_coded} objects to compare. These represent
different "raters" (e.g., different LLM runs, different models, or
human vs. LLM coding). Objects should have the same units (matching \code{.id}
values).}

\item{by}{Character scalar. Name of the variable to compare across raters.
Must be present in all \code{qlm_coded} objects.}

\item{level}{Character scalar. Measurement level of the variable:
\code{"nominal"}, \code{"ordinal"}, \code{"interval"}, or \code{"ratio"}. Default is \code{"nominal"}.
Different sets of agreement statistics are computed for each level.}

\item{tolerance}{Numeric. Tolerance for agreement with numeric data.
Default is 0 (exact agreement required). Used for percent agreement calculation.}
}
\value{
A \code{qlm_comparison} object containing agreement statistics appropriate
for the measurement level:
\describe{
\item{\strong{Nominal level:}}{
\itemize{
\item \code{alpha_nominal}: Krippendorff's alpha
\item \code{kappa}: Cohen's kappa (2 raters) or Fleiss' kappa (3+ raters)
\item \code{kappa_type}: Character indicating "Cohen's" or "Fleiss'"
\item \code{percent_agreement}: Simple percent agreement
}
}
\item{\strong{Ordinal level:}}{
\itemize{
\item \code{alpha_ordinal}: Krippendorff's alpha (ordinal)
\item \code{kappa_weighted}: Weighted kappa (2 raters only)
\item \code{w}: Kendall's W coefficient of concordance
\item \code{rho}: Spearman's rho (average pairwise correlation)
}
}
\item{\strong{Interval/Ratio level:}}{
\itemize{
\item \code{alpha_interval}: Krippendorff's alpha (interval/ratio)
\item \code{icc}: Intraclass correlation coefficient
\item \code{r}: Pearson's r (average pairwise correlation)
}
}
\item{\code{subjects}}{Number of units compared}
\item{\code{raters}}{Number of raters}
\item{\code{level}}{Measurement level}
\item{\code{call}}{The function call}
}
}
\description{
Compares two or more \code{qlm_coded} objects to assess inter-rater reliability
or agreement. This function extracts a specified variable from each coded
result and computes reliability statistics using the irr package.
}
\details{
The function merges the coded objects by their \code{.id} column and only includes
units that are present in all objects. Missing values in any rater will
exclude that unit from analysis.

\strong{Measurement levels and statistics:}
\itemize{
\item \strong{Nominal}: For unordered categories. Computes Krippendorff's alpha,
Cohen's/Fleiss' kappa, and percent agreement.
\item \strong{Ordinal}: For ordered categories. Computes Krippendorff's alpha (ordinal),
weighted kappa (2 raters only), Kendall's W, and Spearman's rho.
\item \strong{Interval/Ratio}: For continuous data. Computes Krippendorff's alpha
(interval/ratio), ICC, and Pearson's r.
}
}
\examples{
\dontrun{
# Compare two LLM coding runs on movie reviews
set.seed(42)
reviews <- data_corpus_LMRDsample[sample(length(data_corpus_LMRDsample), size = 20)]
coded1 <- qlm_code(reviews, data_codebook_sentiment, model = "openai/gpt-4o-mini")
coded2 <- qlm_code(reviews, data_codebook_sentiment,
                   model = "anthropic/claude-sonnet-4-20250514")

# Compare nominal data (polarity: neg/pos)
qlm_compare(coded1, coded2, by = "polarity", level = "nominal")

# Compare ordinal data (rating: 1-10)
qlm_compare(coded1, coded2, by = "rating", level = "ordinal")

# Compare three raters
coded3 <- qlm_code(reviews, data_codebook_sentiment, model = "openai/gpt-4o")
qlm_compare(coded1, coded2, coded3, by = "polarity", level = "nominal")
}

}
\seealso{
\code{\link[=qlm_validate]{qlm_validate()}} for validation of coding against gold standards.
}
