% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/qlm_compare.R
\name{qlm_compare}
\alias{qlm_compare}
\title{Compare coded results for inter-rater reliability}
\usage{
qlm_compare(
  ...,
  by,
  measure = c("alpha", "kappa", "kendall", "agreement"),
  level = c("nominal", "ordinal", "interval", "ratio"),
  tolerance = 0
)
}
\arguments{
\item{...}{Two or more \code{qlm_coded} objects to compare. These represent
different "raters" (e.g., different LLM runs, different models, or
human vs. LLM coding). Objects should have the same units (matching \code{.id}
values).}

\item{by}{Character scalar. Name of the variable to compare across raters.
Must be present in all \code{qlm_coded} objects.}

\item{measure}{Character scalar. The reliability/agreement measure to compute:
\describe{
\item{\code{"alpha"}}{Krippendorff's alpha (default)}
\item{\code{"kappa"}}{Fleiss' kappa (for 3+ raters) or Cohen's kappa (for 2 raters)}
\item{\code{"kendall"}}{Kendall's W coefficient of concordance}
\item{\code{"agreement"}}{Simple percent agreement}
}}

\item{level}{Character scalar. Measurement level of the variable:
\code{"nominal"}, \code{"ordinal"}, \code{"interval"}, or \code{"ratio"}. Default is \code{"nominal"}.}

\item{tolerance}{Numeric. Tolerance for agreement with numeric data.
Default is 0 (exact agreement required).}
}
\value{
A \code{qlm_comparison} object containing:
\describe{
\item{\code{measure}}{The reliability measure used}
\item{\code{value}}{The computed reliability/agreement value}
\item{\code{subjects}}{Number of units compared}
\item{\code{raters}}{Number of raters}
\item{\code{level}}{Measurement level}
\item{\code{detail}}{Original output from the irr package function}
\item{\code{call}}{The function call}
}
}
\description{
Compares two or more \code{qlm_coded} objects to assess inter-rater reliability
or agreement. This function extracts a specified variable from each coded
result and computes reliability statistics using the irr package.
}
\details{
The function merges the coded objects by their \code{.id} column and only includes
units that are present in all objects. Missing values in any rater will
exclude that unit from analysis.
}
\examples{
\dontrun{
# Compare two LLM coding runs on movie reviews
set.seed(42)
reviews <- data_corpus_LMRDsample[sample(length(data_corpus_LMRDsample), size = 20)]
coded1 <- qlm_code(reviews, data_codebook_sentiment, model = "openai/gpt-4o-mini")
coded2 <- qlm_code(reviews, data_codebook_sentiment,
                   model = "anthropic/claude-sonnet-4-20250514")

# Compute agreement for the polarity variable
qlm_compare(coded1, coded2, by = "polarity", measure = "agreement")
qlm_compare(coded1, coded2, by = "polarity", measure = "alpha")

# Compute Krippendorf's alpha for the rating variable
qlm_compare(coded1, coded2, by = "rating", measure = "alpha", level = "ordinal")

# Compare three raters using Fleiss' kappa on polarity
coded3 <- qlm_code(reviews, data_codebook_sentiment, model = "openai/gpt-4o")
qlm_compare(coded1, coded2, coded3, by = "polarity", measure = "kappa", level = "nominal")
}

}
\seealso{
\code{\link[=validate]{validate()}} for validation of coding against gold standards.
}
