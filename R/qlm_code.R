#' Code qualitative data with an LLM
#'
#' Applies a codebook to input data using a large language model, returning
#' a rich object that includes the codebook, execution settings, results, and
#' metadata for reproducibility.
#'
#' Arguments in `...` are dynamically routed to either [ellmer::chat()],
#' [ellmer::parallel_chat_structured()], or [ellmer::batch_chat_structured()]
#' based on their names.
#'
#' @param x Input data: a character vector of texts (for text codebooks) or
#'   file paths to images (for image codebooks). Named vectors will use names
#'   as identifiers in the output; unnamed vectors will use sequential integers.
#' @param codebook A codebook object created with [qlm_codebook()]. Also accepts
#'   deprecated [task()] objects for backward compatibility.
#' @param model Provider (and optionally model) name in the form
#'   `"provider/model"` or `"provider"` (which will use the default model for
#'   that provider). Passed to the `name` argument of [ellmer::chat()].
#'   Examples: `"openai/gpt-4o-mini"`, `"anthropic/claude-3-5-sonnet-20241022"`,
#'   `"ollama/llama3.2"`, `"openai"` (uses default OpenAI model).
#' @param batch Logical. If `TRUE`, uses [ellmer::batch_chat_structured()]
#'   instead of [ellmer::parallel_chat_structured()]. Batch processing is more
#'   cost-effective for large jobs but may have longer turnaround times.
#'   Default is `FALSE`. See [ellmer::batch_chat_structured()] for details.
#' @param ... Additional arguments passed to [ellmer::chat()],
#'   [ellmer::parallel_chat_structured()], or [ellmer::batch_chat_structured()],
#'   based on argument name. Arguments recognized by
#'   [ellmer::parallel_chat_structured()] take priority when there are overlaps.
#'   Batch-specific arguments (`path`, `wait`, `ignore_hash`) are only used when
#'   `batch = TRUE`. Arguments not recognized by any function will generate a warning.
#' @param name Character string identifying this coding run. Default is `"original"`.
#'
#' @details
#' Progress indicators and error handling are provided by the underlying
#' [ellmer::parallel_chat_structured()] or [ellmer::batch_chat_structured()]
#' function. Set `verbose = TRUE` to see progress messages during coding.
#' Retry logic for API failures should be configured through ellmer's options.
#'
#' When `batch = TRUE`, the function uses [ellmer::batch_chat_structured()]
#' which submits jobs to the provider's batch API. This is typically more
#' cost-effective but has longer turnaround times. The `path` argument specifies
#' where batch results are cached, `wait` controls whether to wait for completion,
#' and `ignore_hash` can force reprocessing of cached results.
#'
#' @return A `qlm_coded` object (a tibble with additional attributes):
#'   \describe{
#'     \item{Data columns}{The coded results with a `.id` column for identifiers.}
#'     \item{Attributes}{`data`, `input_type`, and `run` (list containing name, batch, call, codebook, chat_args, execution_args, metadata, parent).}
#'   }
#'   The object prints as a tibble and can be used directly in data manipulation workflows.
#'   The `batch` flag in the `run` attribute indicates whether batch processing was used.
#'   The `execution_args` contains all non-chat execution arguments (for either parallel or batch processing).
#'
#' @seealso
#' [qlm_codebook()] for creating codebooks, [annotate()] for the deprecated function.
#'
#' @examples
#' \dontrun{
#' set.seed(24)
#' texts <- data_corpus_LMRDsample[sample(length(data_corpus_LMRDsample), size = 20)]
#'
#' # Basic sentiment analysis
#' coded <- qlm_code(texts, data_codebook_sentiment, model = "openai")
#' coded  # Print results as tibble
#'
#' # With named inputs (names become IDs in output)
#' texts <- c(doc1 = "Great service!", doc2 = "Very disappointing.")
#' coded <- qlm_code(texts, data_codebook_sentiment, model = "openai")
#'
#' # Specify provider and model
#' coded <- qlm_code(texts, data_codebook_sentiment, model = "openai/gpt-4o-mini")
#'
#' # With execution control
#' coded <- qlm_code(texts, data_codebook_sentiment,
#'                   model = "openai/gpt-4o-mini",
#'                   params = params(temperature = 0))
#'
#' # Include token usage and cost
#' coded <- qlm_code(texts, data_codebook_sentiment,
#'                   model = "openai",
#'                   include_tokens = TRUE,
#'                   include_cost = TRUE)
#' coded
#'
#' # Use batch processing for cost-effective large-scale coding
#' coded_batch <- qlm_code(texts, data_codebook_sentiment,
#'                         model = "openai",
#'                         batch = TRUE,
#'                         path = "batch_results.json",
#'                         ignore_hash = TRUE,
#'                         include_cost = TRUE)
#' coded_batch
#' }
#'
#' @export
qlm_code <- function(x, codebook, model, ..., batch = FALSE, name = "original") {
  # Accept both qlm_codebook and task objects, converting if needed
  if (inherits(codebook, "task") && !inherits(codebook, "qlm_codebook")) {
    codebook <- as_qlm_codebook(codebook)
  }

  if (!inherits(codebook, "qlm_codebook")) {
    cli::cli_abort(c(
      "{.arg codebook} must be created using {.fn qlm_codebook}.",
      "i" = "Use {.fn qlm_codebook} or one of the predefined codebook functions."
    ))
  }

  # Input validation
  if (codebook$input_type == "text" && !is.character(x)) {
    cli::cli_abort("This codebook expects text input (a character vector).")
  }
  if (codebook$input_type == "image" && !is.character(x)) {
    cli::cli_abort("This codebook expects image file paths (a character vector).")
  }

  # Get valid argument names from ellmer functions
  chat_arg_names <- names(formals(ellmer::chat))
  pcs_arg_names <- names(formals(ellmer::parallel_chat_structured))
  batch_arg_names <- names(formals(ellmer::batch_chat_structured))

  # Common model parameters that should go in params
  model_param_names <- c("temperature", "max_tokens", "top_p", "top_k",
                         "frequency_penalty", "presence_penalty", "stop",
                         "seed", "response_format")

  # Route ... arguments
  # All non-chat arguments go to execution_args (for either parallel or batch execution)
  dots <- list(...)
  dot_names <- names(dots)

  chat_args <- dots[dot_names %in% chat_arg_names]

  # execution_args contains everything that's for parallel_chat_structured or batch_chat_structured
  execution_arg_names <- unique(c(pcs_arg_names, batch_arg_names))
  execution_args <- dots[dot_names %in% execution_arg_names]

  # Warn about unrecognized arguments
  all_valid_names <- unique(c(chat_arg_names, execution_arg_names))
  unknown_names <- setdiff(dot_names, all_valid_names)
  if (length(unknown_names) > 0) {
    cli::cli_warn(c(
      "The following {cli::qty(length(unknown_names))} argument{?s} {?was/were} not recognized and {?has/have} been ignored:",
      "x" = "{.arg {unknown_names}}"
    ))
  }

  # Build system prompt from role and instructions
  system_prompt <- if (!is.null(codebook$role)) {
    paste(codebook$role, codebook$instructions, sep = "\n\n")
  } else {
    codebook$instructions
  }

  # Build chat object using ellmer::chat()
  chat <- do.call(ellmer::chat, c(
    list(
      name = model,
      system_prompt = system_prompt
    ),
    chat_args
  ))

  # Prepare prompts based on input type
  if (codebook$input_type == "image") {
    prompts <- lapply(x, ellmer::content_image_file)
  } else {
    prompts <- as.list(x)
  }

  # Execute with appropriate function based on batch parameter
  if (batch) {
    results <- do.call(ellmer::batch_chat_structured, c(
      list(
        chat = chat,
        prompts = prompts,
        type = codebook$schema
      ),
      execution_args
    ))
  } else {
    results <- do.call(ellmer::parallel_chat_structured, c(
      list(
        chat = chat,
        prompts = prompts,
        type = codebook$schema
      ),
      execution_args
    ))
  }

  # Add ID column from input names or sequence
  results$id <- names(x) %||% seq_along(x)

  # Build metadata list
  metadata <- list(
    timestamp = Sys.time(),
    n_units = length(x),
    ellmer_version = tryCatch(
      as.character(utils::packageVersion("ellmer")),
      error = function(e) NA_character_
    ),
    quallmer_version = tryCatch(
      as.character(utils::packageVersion("quallmer")),
      error = function(e) NA_character_
    ),
    R_version = paste(R.version$major, R.version$minor, sep = ".")
  )

  # Add model to chat_args for easy access
  chat_args$name <- model

  # Create and return qlm_coded object
  new_qlm_coded(
    results = results,
    codebook = codebook,
    data = x,
    input_type = codebook$input_type,
    chat_args = chat_args,
    execution_args = execution_args,
    batch = batch,
    metadata = metadata,
    name = name,
    call = match.call(),
    parent = NULL
  )
}

#' Create a qlm_coded object (internal)
#'
#' Low-level constructor for qlm_coded objects. This function is not exported
#' and is intended for internal use by [qlm_code()] and [qlm_replicate()].
#'
#' The object is a tibble with additional qlm_coded class and attributes.
#'
#' @param results Data frame of coded results with id column.
#' @param codebook A qlm_codebook object.
#' @param data The original input data (x from qlm_code).
#' @param input_type Type of input ("text" or "image").
#' @param chat_args List of arguments passed to ellmer::chat().
#' @param execution_args List of arguments passed to ellmer::parallel_chat_structured()
#'   or ellmer::batch_chat_structured(). For backward compatibility, also accepts
#'   pcs_args as an alias.
#' @param batch Logical indicating whether batch processing was used.
#' @param metadata List of metadata (timestamp, versions, etc.).
#' @param name Character string identifying this run.
#' @param call The call that created this object.
#' @param parent Character string identifying parent run (NULL for originals).
#' @param pcs_args Deprecated. Use execution_args instead.
#'
#' @return A qlm_coded object (tibble with attributes).
#' @importFrom utils head
#' @keywords internal
#' @noRd
new_qlm_coded <- function(results, codebook, data, input_type, chat_args,
                           execution_args = NULL, batch = FALSE, metadata,
                           name, call, parent = NULL, pcs_args = NULL) {
  # Backward compatibility: if pcs_args is provided but execution_args is not
  if (is.null(execution_args) && !is.null(pcs_args)) {
    execution_args <- pcs_args
  }
  # Rename id column to .id
  names(results)[names(results) == "id"] <- ".id"

  # Reorder columns to put .id first
  results <- results[, c(".id", setdiff(names(results), ".id"))]

  # Convert to tibble (always available via ellmer)
  results <- tibble::as_tibble(results)

  # Add qlm_coded class and attributes with hierarchical structure
  structure(
    results,
    class = c("qlm_coded", class(results)),
    data = data,
    input_type = input_type,
    run = list(
      name = name,
      batch = batch,
      call = call,
      codebook = codebook,
      chat_args = chat_args,
      execution_args = execution_args,
      metadata = metadata,
      parent = parent
    )
  )
}


#' Print a qlm_coded object
#'
#' @param x A qlm_coded object.
#' @param ... Additional arguments passed to print methods.
#'
#' @return Invisibly returns the input object \code{x}. Called for side effects (printing to console).
#' @keywords internal
#' @export
print.qlm_coded <- function(x, ...) {
  run <- attr(x, "run")

  # Print header
  cat("# quallmer coded object\n")
  cat("# Run:      ", run$name, "\n", sep = "")

  # Distinguish human vs LLM coding
  if (!is.null(run$metadata$source) && run$metadata$source == "human") {
    cat("# Source:   Human coder\n")
    if (!is.null(run$codebook$name) && run$codebook$name != "Human-coded data") {
      cat("# Codebook: ", run$codebook$name, "\n", sep = "")
    }
  } else {
    cat("# Codebook: ", run$codebook$name, "\n", sep = "")
    cat("# Model:    ", run$chat_args$name %||% "unknown", "\n", sep = "")
  }

  cat("# Units:    ", run$metadata$n_units, "\n", sep = "")

  if (!is.null(run$parent)) {
    cat("# Parent:   ", run$parent, "\n", sep = "")
  }

  cat("\n")

  # Print data using parent class method
  NextMethod()
}

